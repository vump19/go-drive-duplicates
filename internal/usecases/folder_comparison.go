package usecases

import (
	"context"
	"fmt"
	"go-drive-duplicates/internal/domain/entities"
	"go-drive-duplicates/internal/domain/repositories"
	"go-drive-duplicates/internal/domain/services"
	"log"
	"regexp"
	"sync"
)

// FolderComparisonUseCase handles folder comparison operations
type FolderComparisonUseCase struct {
	fileRepo          repositories.FileRepository
	comparisonRepo    repositories.ComparisonRepository
	progressRepo      repositories.ProgressRepository
	storageProvider   services.StorageProvider
	hashService       services.HashService
	comparisonService services.ComparisonService
	progressService   services.ProgressService

	// Configuration
	workerCount       int
	includeSubfolders bool
	deepComparison    bool
	minFileSize       int64
}

// NewFolderComparisonUseCase creates a new folder comparison use case
func NewFolderComparisonUseCase(
	fileRepo repositories.FileRepository,
	comparisonRepo repositories.ComparisonRepository,
	progressRepo repositories.ProgressRepository,
	storageProvider services.StorageProvider,
	hashService services.HashService,
	comparisonService services.ComparisonService,
	progressService services.ProgressService,
) *FolderComparisonUseCase {
	return &FolderComparisonUseCase{
		fileRepo:          fileRepo,
		comparisonRepo:    comparisonRepo,
		progressRepo:      progressRepo,
		storageProvider:   storageProvider,
		hashService:       hashService,
		comparisonService: comparisonService,
		progressService:   progressService,
		workerCount:       5,
		includeSubfolders: true,
		deepComparison:    true,
		minFileSize:       0,
	}
}

// CompareFoldersRequest represents the request for comparing folders
type CompareFoldersRequest struct {
	SourceFolderID      string                   `json:"sourceFolderId"`
	TargetFolderID      string                   `json:"targetFolderId"`
	IncludeSubfolders   bool                     `json:"includeSubfolders"`
	DeepComparison      bool                     `json:"deepComparison"`
	ForceNewComparison  bool                     `json:"forceNewComparison"`  // ê¸°ì¡´ ê²°ê³¼ ë¬´ì‹œí•˜ê³  ìƒˆë¡œ ë¹„êµ
	MinFileSize         int64                    `json:"minFileSize,omitempty"`
	WorkerCount         int                      `json:"workerCount,omitempty"`
	ResumeProgressID    int                      `json:"resumeProgressId,omitempty"` // ì¬ê°œí•  ì§„í–‰ ìƒí™© ID
	ProgressCallback    func(*entities.Progress) `json:"-"`
}

// ResumeComparisonRequest represents the request for resuming folder comparison
type ResumeComparisonRequest struct {
	ProgressID int `json:"progressId"`
}

// CompareFoldersResponse represents the response for comparing folders
type CompareFoldersResponse struct {
	Progress         *entities.Progress         `json:"progress"`
	ComparisonResult *entities.ComparisonResult `json:"comparisonResult"`
	Errors           []string                   `json:"errors,omitempty"`
}

// GetComparisonProgressRequest represents the request for getting comparison progress
type GetComparisonProgressRequest struct {
	ComparisonID int `json:"comparisonId"`
}

// LoadSavedComparisonRequest represents the request for loading a saved comparison
type LoadSavedComparisonRequest struct {
	SourceFolderID string `json:"sourceFolderId"`
	TargetFolderID string `json:"targetFolderId"`
}

// CompareFolders compares two folders and finds duplicate files
func (uc *FolderComparisonUseCase) CompareFolders(ctx context.Context, req *CompareFoldersRequest) (*CompareFoldersResponse, error) {
	log.Printf("ğŸ“‚ í´ë” ë¹„êµ ì‹œì‘: %s vs %s", req.SourceFolderID, req.TargetFolderID)

	// Apply configuration
	if req.WorkerCount > 0 {
		uc.workerCount = req.WorkerCount
	}
	uc.includeSubfolders = req.IncludeSubfolders
	uc.deepComparison = req.DeepComparison
	if req.MinFileSize > 0 {
		uc.minFileSize = req.MinFileSize
	}

	// ì¬ê°œ ìš”ì²­ì¸ ê²½ìš° ê¸°ì¡´ ì§„í–‰ ìƒí™© í™•ì¸
	if req.ResumeProgressID > 0 {
		return uc.resumeComparison(ctx, req)
	}

	// Validate folder access
	if err := uc.validateFolderAccess(ctx, req.SourceFolderID, req.TargetFolderID); err != nil {
		return nil, fmt.Errorf("í´ë” ì ‘ê·¼ ê¶Œí•œ í™•ì¸ ì‹¤íŒ¨: %w", err)
	}

	// Check for existing comparison (only if not forced to create new)
	if !req.ForceNewComparison {
		existingComparison, err := uc.comparisonRepo.GetByFolders(ctx, req.SourceFolderID, req.TargetFolderID)
		if err == nil && existingComparison != nil {
			log.Printf("ğŸ“‹ ê¸°ì¡´ ë¹„êµ ê²°ê³¼ ë°œê²¬: ID %d", existingComparison.ID)
			return &CompareFoldersResponse{
				ComparisonResult: existingComparison,
				Errors:           make([]string, 0),
			}, nil
		}
	} else {
		log.Printf("ğŸ”„ ìƒˆë¡œìš´ ë¹„êµ ê°•ì œ ì‹¤í–‰ - ê¸°ì¡´ ê²°ê³¼ ë¬´ì‹œ")
	}

	// Create progress tracker with checkpoint metadata
	progress, err := uc.progressService.StartOperation(ctx, entities.OperationFolderComparison, 0)
	if err != nil {
		return nil, fmt.Errorf("ì§„í–‰ ìƒí™© ìƒì„± ì‹¤íŒ¨: %w", err)
	}

	// ì²´í¬í¬ì¸íŠ¸ ë©”íƒ€ë°ì´í„° ì €ì¥
	progress.SetMetadata("sourceFolderId", req.SourceFolderID)
	progress.SetMetadata("targetFolderId", req.TargetFolderID)
	progress.SetMetadata("includeSubfolders", req.IncludeSubfolders)
	progress.SetMetadata("deepComparison", req.DeepComparison)
	progress.SetMetadata("minFileSize", req.MinFileSize)
	progress.SetMetadata("workerCount", uc.workerCount)
	progress.SetMetadata("currentPhase", "initialized")

	// Get folder names
	sourceFolderName, targetFolderName := uc.getFolderNames(ctx, req.SourceFolderID, req.TargetFolderID)

	// Initialize comparison result
	comparisonResult := entities.NewComparisonResult(
		req.SourceFolderID,
		req.TargetFolderID,
		sourceFolderName,
		targetFolderName,
	)

	// Initialize response
	response := &CompareFoldersResponse{
		Progress:         progress,
		ComparisonResult: comparisonResult,
		Errors:           make([]string, 0),
	}

	// Start comparison in background with a new context (not tied to HTTP request)
	go uc.performFolderComparison(context.Background(), req, progress, comparisonResult, response)

	return response, nil
}

// GetComparisonProgress returns the current comparison progress
func (uc *FolderComparisonUseCase) GetComparisonProgress(ctx context.Context, req *GetComparisonProgressRequest) (*entities.Progress, error) {
	return uc.progressService.GetProgress(ctx, req.ComparisonID)
}

// ResumeComparison resumes a paused or failed comparison
func (uc *FolderComparisonUseCase) ResumeComparison(ctx context.Context, req *ResumeComparisonRequest) (*CompareFoldersResponse, error) {
	log.Printf("ğŸ”„ í´ë” ë¹„êµ ì¬ê°œ: Progress ID %d", req.ProgressID)

	// Get existing progress
	progress, err := uc.progressService.GetProgress(ctx, req.ProgressID)
	if err != nil {
		return nil, fmt.Errorf("ì§„í–‰ ìƒí™© ì¡°íšŒ ì‹¤íŒ¨: %w", err)
	}

	if progress.Status == entities.StatusCompleted {
		return nil, fmt.Errorf("ì´ë¯¸ ì™„ë£Œëœ ì‘ì—…ì…ë‹ˆë‹¤")
	}

	// Extract metadata to reconstruct request
	sourceFolderID, _ := progress.GetMetadata("sourceFolderId")
	targetFolderID, _ := progress.GetMetadata("targetFolderId")
	includeSubfolders, _ := progress.GetMetadata("includeSubfolders")
	deepComparison, _ := progress.GetMetadata("deepComparison")
	minFileSize, _ := progress.GetMetadata("minFileSize")
	workerCount, _ := progress.GetMetadata("workerCount")

	// Reconstruct request
	resumeReq := &CompareFoldersRequest{
		SourceFolderID:    sourceFolderID.(string),
		TargetFolderID:    targetFolderID.(string),
		IncludeSubfolders: includeSubfolders.(bool),
		DeepComparison:    deepComparison.(bool),
		MinFileSize:       int64(minFileSize.(float64)),
		WorkerCount:       int(workerCount.(float64)),
		ResumeProgressID:  req.ProgressID,
	}

	return uc.resumeComparison(ctx, resumeReq)
}

// resumeComparison handles the actual resumption logic
func (uc *FolderComparisonUseCase) resumeComparison(ctx context.Context, req *CompareFoldersRequest) (*CompareFoldersResponse, error) {
	// Get existing progress
	progress, err := uc.progressService.GetProgress(ctx, req.ResumeProgressID)
	if err != nil {
		return nil, fmt.Errorf("ì§„í–‰ ìƒí™© ì¡°íšŒ ì‹¤íŒ¨: %w", err)
	}

	// Get folder names
	sourceFolderName, targetFolderName := uc.getFolderNames(ctx, req.SourceFolderID, req.TargetFolderID)

	// Check if comparison result already exists
	comparisonResult, err := uc.comparisonRepo.GetByFolders(ctx, req.SourceFolderID, req.TargetFolderID)
	if err != nil {
		// Create new comparison result
		comparisonResult = entities.NewComparisonResult(
			req.SourceFolderID,
			req.TargetFolderID,
			sourceFolderName,
			targetFolderName,
		)
	}

	// Initialize response
	response := &CompareFoldersResponse{
		Progress:         progress,
		ComparisonResult: comparisonResult,
		Errors:           make([]string, 0),
	}

	// Resume progress
	progress.Resume()
	uc.progressService.UpdateOperation(ctx, progress.ID, progress.ProcessedItems, "ì‘ì—… ì¬ê°œ ì¤‘...")

	// Continue comparison from checkpoint
	go uc.performFolderComparison(context.Background(), req, progress, comparisonResult, response)

	return response, nil
}

// GetPendingComparisons returns all pending/failed comparison operations
func (uc *FolderComparisonUseCase) GetPendingComparisons(ctx context.Context) ([]*entities.Progress, error) {
	// For now, return empty list - this can be implemented later when needed
	log.Printf("ğŸ“‹ ì¤‘ë‹¨ëœ ì‘ì—… ì¡°íšŒ ìš”ì²­ (í˜„ì¬ ë¯¸êµ¬í˜„)")
	return []*entities.Progress{}, nil
}

// LoadSavedComparison loads a previously saved comparison result
func (uc *FolderComparisonUseCase) LoadSavedComparison(ctx context.Context, req *LoadSavedComparisonRequest) (*entities.ComparisonResult, error) {
	comparison, err := uc.comparisonRepo.GetByFolders(ctx, req.SourceFolderID, req.TargetFolderID)
	if err != nil {
		return nil, fmt.Errorf("ì €ì¥ëœ ë¹„êµ ê²°ê³¼ ì¡°íšŒ ì‹¤íŒ¨: %w", err)
	}

	if comparison == nil {
		return nil, fmt.Errorf("ì €ì¥ëœ ë¹„êµ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
	}

	return comparison, nil
}

// DeleteComparisonResult deletes a comparison result
func (uc *FolderComparisonUseCase) DeleteComparisonResult(ctx context.Context, comparisonID int) error {
	return uc.comparisonRepo.Delete(ctx, comparisonID)
}

// GetRecentComparisons returns recent comparison results
func (uc *FolderComparisonUseCase) GetRecentComparisons(ctx context.Context, limit int) ([]*entities.ComparisonResult, error) {
	return uc.comparisonRepo.GetRecentComparisons(ctx, limit)
}

// performFolderComparison performs the actual folder comparison
func (uc *FolderComparisonUseCase) performFolderComparison(ctx context.Context, req *CompareFoldersRequest, progress *entities.Progress, result *entities.ComparisonResult, response *CompareFoldersResponse) {
	log.Printf("ğŸš€ ë°±ê·¸ë¼ìš´ë“œ í´ë” ë¹„êµ ì‘ì—… ì‹œì‘ - Progress ID: %d", progress.ID)
	
	defer func() {
		if r := recover(); r != nil {
			log.Printf("ğŸ’¥ í´ë” ë¹„êµ ì¤‘ íŒ¨ë‹‰ ë°œìƒ: %v", r)
			log.Printf("ğŸ“ íŒ¨ë‹‰ ìœ„ì¹˜ - Progress ID: %d, í˜„ì¬ ë‹¨ê³„: %v", progress.ID, progress.CurrentStep)
			uc.progressService.FailOperation(ctx, progress.ID, fmt.Sprintf("íŒ¨ë‹‰ ë°œìƒ: %v", r))
		}
		log.Printf("ğŸ”š ë°±ê·¸ë¼ìš´ë“œ í´ë” ë¹„êµ ì‘ì—… ì¢…ë£Œ - Progress ID: %d", progress.ID)
	}()

	// Update progress to running
	log.Printf("â–¶ï¸ ì§„í–‰ ìƒíƒœë¥¼ ì‹¤í–‰ ì¤‘ìœ¼ë¡œ ë³€ê²½...")
	progress.Start()
	uc.progressService.UpdateOperation(ctx, progress.ID, 0, "í´ë” ë¹„êµ ì‹œì‘...")
	log.Printf("âœ… ì§„í–‰ ìƒíƒœ ì—…ë°ì´íŠ¸ ì™„ë£Œ: %s", progress.Status)

	// Check current phase and resume from checkpoint
	currentPhase, _ := progress.GetMetadata("currentPhase")
	var sourceFiles, targetFiles []*entities.File
	var err error

	// Step 1: Get files from source folder (skip if already done)
	if currentPhase == "initialized" || currentPhase == "" {
		uc.progressService.UpdateOperation(ctx, progress.ID, 0, "ê¸°ì¤€ í´ë” íŒŒì¼ ì¡°íšŒ ì¤‘...")
		progress.SetMetadata("currentPhase", "scanning_source")
		
		log.Printf("ğŸ“‚ ê¸°ì¤€ í´ë” ìŠ¤ìº” ì‹œì‘ - Folder ID: %s, í•˜ìœ„í´ë” í¬í•¨: %v", req.SourceFolderID, req.IncludeSubfolders)
		sourceFiles, err = uc.getFilesFromFolder(ctx, req.SourceFolderID, req.IncludeSubfolders)
		if err != nil {
			log.Printf("âŒ ê¸°ì¤€ í´ë” íŒŒì¼ ì¡°íšŒ ì‹¤íŒ¨: %v", err)
			uc.progressService.FailOperation(ctx, progress.ID, fmt.Sprintf("ê¸°ì¤€ í´ë” íŒŒì¼ ì¡°íšŒ ì‹¤íŒ¨: %v", err))
			response.Errors = append(response.Errors, fmt.Sprintf("ê¸°ì¤€ í´ë” íŒŒì¼ ì¡°íšŒ ì‹¤íŒ¨: %v", err))
			return
		}
		
		// Save checkpoint - source files scanned
		progress.SetMetadata("sourceFileCount", len(sourceFiles))
		progress.SetMetadata("currentPhase", "source_completed")
		log.Printf("ğŸ“‚ ê¸°ì¤€ í´ë” ìŠ¤ìº” ì™„ë£Œ: %dê°œ íŒŒì¼ ë°œê²¬", len(sourceFiles))
		
		// Important: Save progress immediately to persist the checkpoint
		err = uc.progressRepo.Update(ctx, progress)
		if err != nil {
			log.Printf("âš ï¸ ì§„í–‰ ìƒíƒœ ì €ì¥ ì‹¤íŒ¨: %v", err)
		}
		
	} else if currentPhase == "source_completed" || currentPhase == "scanning_target" {
		log.Printf("ğŸ”„ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ê°œ: %s", currentPhase)
		// Reload source files if resuming
		sourceFiles, err = uc.getFilesFromFolder(ctx, req.SourceFolderID, req.IncludeSubfolders)
		if err != nil {
			log.Printf("âŒ ê¸°ì¤€ í´ë” íŒŒì¼ ì¬ì¡°íšŒ ì‹¤íŒ¨: %v", err)
			uc.progressService.FailOperation(ctx, progress.ID, fmt.Sprintf("ê¸°ì¤€ í´ë” íŒŒì¼ ì¬ì¡°íšŒ ì‹¤íŒ¨: %v", err))
			response.Errors = append(response.Errors, fmt.Sprintf("ê¸°ì¤€ í´ë” íŒŒì¼ ì¬ì¡°íšŒ ì‹¤íŒ¨: %v", err))
			return
		}
	}

	// Step 2: Get files from target folder (skip if already done)
	log.Printf("ğŸ” ëŒ€ìƒ í´ë” ìŠ¤ìº” ì¡°ê±´ í™•ì¸ - currentPhase: '%s'", currentPhase)
	// If we just completed source scanning or are resuming target scanning, proceed with target folder
	if currentPhase == "source_completed" || currentPhase == "scanning_target" || (currentPhase == "initialized" && len(sourceFiles) > 0) {
		log.Printf("âœ… ëŒ€ìƒ í´ë” ìŠ¤ìº” ì¡°ê±´ í†µê³¼")
		uc.progressService.UpdateOperation(ctx, progress.ID, 0, "ëŒ€ìƒ í´ë” íŒŒì¼ ì¡°íšŒ ì¤‘...")
		progress.SetMetadata("currentPhase", "scanning_target")
		
		// Save progress immediately to persist the phase change
		err = uc.progressRepo.Update(ctx, progress)
		if err != nil {
			log.Printf("âš ï¸ ìŠ¤ìº” ìƒíƒœ ì €ì¥ ì‹¤íŒ¨: %v", err)
		}
		
		log.Printf("ğŸ¯ ëŒ€ìƒ í´ë” ìŠ¤ìº” ì‹œì‘ - Folder ID: %s, í•˜ìœ„í´ë” í¬í•¨: %v", req.TargetFolderID, req.IncludeSubfolders)
		targetFiles, err = uc.getFilesFromFolder(ctx, req.TargetFolderID, req.IncludeSubfolders)
		if err != nil {
			log.Printf("âŒ ëŒ€ìƒ í´ë” íŒŒì¼ ì¡°íšŒ ì‹¤íŒ¨: %v", err)
			uc.progressService.FailOperation(ctx, progress.ID, fmt.Sprintf("ëŒ€ìƒ í´ë” íŒŒì¼ ì¡°íšŒ ì‹¤íŒ¨: %v", err))
			response.Errors = append(response.Errors, fmt.Sprintf("ëŒ€ìƒ í´ë” íŒŒì¼ ì¡°íšŒ ì‹¤íŒ¨: %v", err))
			return
		}
		
		// Save checkpoint - target files scanned
		progress.SetMetadata("targetFileCount", len(targetFiles))
		progress.SetMetadata("currentPhase", "target_completed")
		log.Printf("ğŸ¯ ëŒ€ìƒ í´ë” ìŠ¤ìº” ì™„ë£Œ: %dê°œ íŒŒì¼ ë°œê²¬", len(targetFiles))
		
		// Important: Save progress immediately to persist the checkpoint
		err = uc.progressRepo.Update(ctx, progress)
		if err != nil {
			log.Printf("âš ï¸ ëŒ€ìƒ í´ë” ìŠ¤ìº” ì™„ë£Œ ìƒíƒœ ì €ì¥ ì‹¤íŒ¨: %v", err)
		}
	} else {
		log.Printf("âš ï¸ ëŒ€ìƒ í´ë” ìŠ¤ìº” ì¡°ê±´ ë¶ˆì¼ì¹˜ - currentPhase: '%s'", currentPhase)
	}
	
	// Handle other phases or continue processing
	if currentPhase != "initialized" && currentPhase != "scanning_source" && currentPhase != "source_completed" && currentPhase != "scanning_target" && currentPhase != "target_completed" {
		// Load both source and target files if resuming from later phase
		sourceFiles, err = uc.getFilesFromFolder(ctx, req.SourceFolderID, req.IncludeSubfolders)
		if err != nil {
			log.Printf("âŒ ê¸°ì¤€ í´ë” íŒŒì¼ ì¬ì¡°íšŒ ì‹¤íŒ¨: %v", err)
			uc.progressService.FailOperation(ctx, progress.ID, fmt.Sprintf("ê¸°ì¤€ í´ë” íŒŒì¼ ì¬ì¡°íšŒ ì‹¤íŒ¨: %v", err))
			response.Errors = append(response.Errors, fmt.Sprintf("ê¸°ì¤€ í´ë” íŒŒì¼ ì¬ì¡°íšŒ ì‹¤íŒ¨: %v", err))
			return
		}
		
		targetFiles, err = uc.getFilesFromFolder(ctx, req.TargetFolderID, req.IncludeSubfolders)
		if err != nil {
			log.Printf("âŒ ëŒ€ìƒ í´ë” íŒŒì¼ ì¬ì¡°íšŒ ì‹¤íŒ¨: %v", err)
			uc.progressService.FailOperation(ctx, progress.ID, fmt.Sprintf("ëŒ€ìƒ í´ë” íŒŒì¼ ì¬ì¡°íšŒ ì‹¤íŒ¨: %v", err))
			response.Errors = append(response.Errors, fmt.Sprintf("ëŒ€ìƒ í´ë” íŒŒì¼ ì¬ì¡°íšŒ ì‹¤íŒ¨: %v", err))
			return
		}
	}

	log.Printf("ğŸ“Š ê¸°ì¤€ í´ë”: %dê°œ íŒŒì¼, ëŒ€ìƒ í´ë”: %dê°œ íŒŒì¼", len(sourceFiles), len(targetFiles))

	// Update folder statistics
	sourceTotalSize := uc.calculateTotalSize(sourceFiles)
	targetTotalSize := uc.calculateTotalSize(targetFiles)
	result.SetSourceStats(len(sourceFiles), sourceTotalSize)
	result.SetTargetStats(len(targetFiles), targetTotalSize)

	// Step 3: Calculate hashes if deep comparison is enabled (skip if already done)
	totalFiles := len(sourceFiles) + len(targetFiles)
	progress.SetTotal(totalFiles)

	if req.DeepComparison && (currentPhase == "target_completed" || currentPhase == "calculating_hashes") {
		uc.progressService.UpdateOperation(ctx, progress.ID, 0, "íŒŒì¼ í•´ì‹œ ê³„ì‚° ì¤‘...")
		progress.SetMetadata("currentPhase", "calculating_hashes")
		
		allFiles := append(sourceFiles, targetFiles...)
		err := uc.calculateHashesForFiles(ctx, allFiles, progress, req.ProgressCallback)
		if err != nil {
			log.Printf("âŒ í•´ì‹œ ê³„ì‚° ì‹¤íŒ¨: %v", err)
			response.Errors = append(response.Errors, fmt.Sprintf("í•´ì‹œ ê³„ì‚° ì‹¤íŒ¨: %v", err))
		} else {
			// Save checkpoint - hash calculation completed
			progress.SetMetadata("currentPhase", "hashes_completed")
			log.Printf("ğŸ” ì²´í¬í¬ì¸íŠ¸ ì €ì¥: í•´ì‹œ ê³„ì‚° ì™„ë£Œ")
		}
	} else if currentPhase == "hashes_completed" || currentPhase == "comparing_files" {
		log.Printf("ğŸ”„ í•´ì‹œ ê³„ì‚° ë‹¨ê³„ ê±´ë„ˆë›°ê¸° (ì´ë¯¸ ì™„ë£Œë¨)")
	}

	// Step 4: Compare files and find duplicates (final phase)
	if currentPhase != "completed" {
		uc.progressService.UpdateOperation(ctx, progress.ID, totalFiles, "ì¤‘ë³µ íŒŒì¼ ê²€ìƒ‰ ì¤‘...")
		progress.SetMetadata("currentPhase", "comparing_files")
		
		duplicateFiles := uc.findDuplicatesBetweenFolders(sourceFiles, targetFiles, req.DeepComparison)

		// Add duplicate files to result
		for _, file := range duplicateFiles {
			result.AddDuplicateFile(file)
		}

		log.Printf("ğŸ“Š ì¤‘ë³µ íŒŒì¼ %dê°œ ë°œê²¬ (%.1f%% ì¤‘ë³µ)",
			len(duplicateFiles), result.DuplicationPercentage)

			// Step 5a: Save files to database first to avoid foreign key constraint errors
		log.Printf("ğŸ’¾ íŒŒì¼ ë©”íƒ€ë°ì´í„° ì €ì¥ ì‹œì‘...")
		uc.progressService.UpdateOperation(ctx, progress.ID, totalFiles, "íŒŒì¼ ë©”íƒ€ë°ì´í„° ì €ì¥ ì¤‘...")
		
		allFiles := append(sourceFiles, targetFiles...)
		err = uc.saveFilesToDatabase(ctx, allFiles)
		if err != nil {
			log.Printf("âŒ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: %v", err)
			uc.progressService.FailOperation(ctx, progress.ID, fmt.Sprintf("íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: %v", err))
			response.Errors = append(response.Errors, fmt.Sprintf("íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: %v", err))
			return
		}
		log.Printf("âœ… íŒŒì¼ ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ")
		
		// Step 5b: Save comparison result
		log.Printf("ğŸ’¾ ë¹„êµ ê²°ê³¼ ì €ì¥ ì‹œì‘...")
		uc.progressService.UpdateOperation(ctx, progress.ID, totalFiles, "ë¹„êµ ê²°ê³¼ ì €ì¥ ì¤‘...")
		progress.SetMetadata("currentPhase", "saving_results")
		
		log.Printf("ğŸ“Š ì €ì¥í•  ë°ì´í„°: ì¤‘ë³µ íŒŒì¼ %dê°œ, ê¸°ì¤€ í´ë” %dê°œ íŒŒì¼, ëŒ€ìƒ í´ë” %dê°œ íŒŒì¼", 
			len(duplicateFiles), len(sourceFiles), len(targetFiles))
		
		err = uc.comparisonRepo.Save(ctx, result)
		if err != nil {
			log.Printf("âŒ ë¹„êµ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: %v", err)
			uc.progressService.FailOperation(ctx, progress.ID, fmt.Sprintf("ë¹„êµ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: %v", err))
			response.Errors = append(response.Errors, fmt.Sprintf("ë¹„êµ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: %v", err))
			return
		} else {
			log.Printf("âœ… ë¹„êµ ê²°ê³¼ ì €ì¥ ì™„ë£Œ")
			// Final checkpoint - comparison completed
			progress.SetMetadata("currentPhase", "completed")
			log.Printf("âœ… ì²´í¬í¬ì¸íŠ¸ ì €ì¥: í´ë” ë¹„êµ ì™„ë£Œ")
		}
	}

	// Complete the operation
	log.Printf("ğŸ ì‘ì—… ì™„ë£Œ ì²˜ë¦¬ ì‹œì‘...")
	uc.progressService.CompleteOperation(ctx, progress.ID)
	progress.Complete()
	log.Printf("âœ… ì§„í–‰ ìƒí™© ì™„ë£Œ ì²˜ë¦¬ë¨")

	if req.ProgressCallback != nil {
		req.ProgressCallback(progress)
		log.Printf("ğŸ“ ì§„í–‰ ìƒí™© ì½œë°± í˜¸ì¶œë¨")
	}

	log.Printf("ğŸ‰ í´ë” ë¹„êµ ìµœì¢… ì™„ë£Œ: %s ì ˆì•½ ê°€ëŠ¥, í´ë” ì‚­ì œ ê¶Œì¥: %v",
		formatFileSize(result.GetWastedSpace()), result.CanDeleteTargetFolder)
	log.Printf("ğŸ“ˆ ìµœì¢… í†µê³„: ê¸°ì¤€ í´ë” %dê°œ íŒŒì¼, ëŒ€ìƒ í´ë” %dê°œ íŒŒì¼, ì¤‘ë³µë¥  %.1f%%",
		len(sourceFiles), len(targetFiles), result.DuplicationPercentage)
}

// validateFolderAccess validates access to both folders
func (uc *FolderComparisonUseCase) validateFolderAccess(ctx context.Context, sourceFolderID, targetFolderID string) error {
	// Check source folder
	_, err := uc.storageProvider.GetFolder(ctx, sourceFolderID)
	if err != nil {
		return fmt.Errorf("ê¸°ì¤€ í´ë” ì ‘ê·¼ ì‹¤íŒ¨ [%s]: %w", sourceFolderID, err)
	}

	// Check target folder
	_, err = uc.storageProvider.GetFolder(ctx, targetFolderID)
	if err != nil {
		return fmt.Errorf("ëŒ€ìƒ í´ë” ì ‘ê·¼ ì‹¤íŒ¨ [%s]: %w", targetFolderID, err)
	}

	return nil
}

// getFolderNames retrieves folder names for display
func (uc *FolderComparisonUseCase) getFolderNames(ctx context.Context, sourceFolderID, targetFolderID string) (string, string) {
	sourceName := "ì•Œ ìˆ˜ ì—†ëŠ” í´ë”"
	targetName := "ì•Œ ìˆ˜ ì—†ëŠ” í´ë”"

	if sourceFolder, err := uc.storageProvider.GetFolder(ctx, sourceFolderID); err == nil {
		sourceName = sourceFolder.Name
	}

	if targetFolder, err := uc.storageProvider.GetFolder(ctx, targetFolderID); err == nil {
		targetName = targetFolder.Name
	}

	return sourceName, targetName
}

// getFilesFromFolder retrieves files from a folder
func (uc *FolderComparisonUseCase) getFilesFromFolder(ctx context.Context, folderID string, includeSubfolders bool) ([]*entities.File, error) {
	if includeSubfolders {
		return uc.getFilesRecursive(ctx, folderID)
	}
	return uc.storageProvider.ListFiles(ctx, folderID)
}

// getFilesRecursive recursively gets files from folder and subfolders
func (uc *FolderComparisonUseCase) getFilesRecursive(ctx context.Context, folderID string) ([]*entities.File, error) {
	var allFiles []*entities.File

	// Get files in current folder
	log.Printf("ğŸ“‚ í´ë” ìŠ¤ìº” ì‹œì‘: %s", folderID)
	files, err := uc.storageProvider.ListFiles(ctx, folderID)
	if err != nil {
		log.Printf("âŒ í´ë” íŒŒì¼ ì¡°íšŒ ì‹¤íŒ¨ [%s]: %v", folderID, err)
		return nil, err
	}
	log.Printf("ğŸ“‹ í´ë” [%s]ì—ì„œ ë°œê²¬ëœ í•­ëª©: %dê°œ", folderID, len(files))
	
	// Debug: Log each item to see what we're getting
	for i, file := range files {
		log.Printf("  í•­ëª© %d: %s (íƒ€ì…: %s, í¬ê¸°: %d)", i+1, file.Name, file.MimeType, file.Size)
	}

	// Separate files and folders
	var actualFiles []*entities.File
	var subfolders []*entities.File

	for _, file := range files {
		if file.GetFileCategory() == "folder" {
			log.Printf("ğŸ“ í•˜ìœ„ í´ë” ë°œê²¬: %s (%s)", file.Name, file.ID)
			subfolders = append(subfolders, file)
		} else if file.Size >= uc.minFileSize {
			log.Printf("ğŸ“„ íŒŒì¼ ë°œê²¬: %s (í¬ê¸°: %d bytes)", file.Name, file.Size)
			actualFiles = append(actualFiles, file)
		} else {
			log.Printf("â­ï¸ íŒŒì¼ ê±´ë„ˆë›°ê¸° (ìµœì†Œ í¬ê¸° ë¯¸ë§Œ): %s (í¬ê¸°: %d bytes)", file.Name, file.Size)
		}
	}

	log.Printf("ğŸ“Š í˜„ì¬ í´ë” [%s]: íŒŒì¼ %dê°œ, í•˜ìœ„ í´ë” %dê°œ", folderID, len(actualFiles), len(subfolders))

	// Add current folder files
	allFiles = append(allFiles, actualFiles...)

	// Recursively get files from subfolders
	for _, subfolder := range subfolders {
		log.Printf("ğŸ” í•˜ìœ„ í´ë” ì¬ê·€ ìŠ¤ìº” ì‹œì‘: %s (%s)", subfolder.Name, subfolder.ID)
		subFiles, err := uc.getFilesRecursive(ctx, subfolder.ID)
		if err != nil {
			log.Printf("âš ï¸ í•˜ìœ„ í´ë” íŒŒì¼ ì¡°íšŒ ì‹¤íŒ¨ [%s]: %v", subfolder.ID, err)
			continue
		}
		log.Printf("âœ… í•˜ìœ„ í´ë” [%s]ì—ì„œ %dê°œ íŒŒì¼ ë°œê²¬", subfolder.Name, len(subFiles))
		allFiles = append(allFiles, subFiles...)
	}

	log.Printf("ğŸ¯ í´ë” [%s] ìµœì¢… ê²°ê³¼: ì´ %dê°œ íŒŒì¼", folderID, len(allFiles))
	return allFiles, nil
}

// calculateTotalSize calculates total size of files
func (uc *FolderComparisonUseCase) calculateTotalSize(files []*entities.File) int64 {
	var total int64
	for _, file := range files {
		total += file.Size
	}
	return total
}

// calculateHashesForFiles calculates hashes for files that don't have them
func (uc *FolderComparisonUseCase) calculateHashesForFiles(ctx context.Context, files []*entities.File, progress *entities.Progress, callback func(*entities.Progress)) error {
	// Find files that need hash calculation
	filesNeedingHash := make([]*entities.File, 0)
	for _, file := range files {
		if !file.IsHashCalculated() {
			filesNeedingHash = append(filesNeedingHash, file)
		}
	}

	if len(filesNeedingHash) == 0 {
		log.Println("âœ… ëª¨ë“  íŒŒì¼ì˜ í•´ì‹œê°€ ì´ë¯¸ ê³„ì‚°ë˜ì–´ ìˆìŠµë‹ˆë‹¤")
		return nil
	}

	log.Printf("ğŸ” %dê°œ íŒŒì¼ì˜ í•´ì‹œ ê³„ì‚° ì‹œì‘", len(filesNeedingHash))

	// Use worker pool for parallel hash calculation
	jobs := make(chan *entities.File, len(filesNeedingHash))
	results := make(chan error, len(filesNeedingHash))

	// Start workers
	var wg sync.WaitGroup
	for w := 0; w < uc.workerCount; w++ {
		wg.Add(1)
		go func() {
			defer wg.Done()
			for file := range jobs {
				err := uc.calculateFileHash(ctx, file)
				results <- err
			}
		}()
	}

	// Send jobs
	for _, file := range filesNeedingHash {
		jobs <- file
	}
	close(jobs)

	// Wait for workers to complete
	go func() {
		wg.Wait()
		close(results)
	}()

	// Collect results
	processed := 0
	errors := make([]string, 0)

	for err := range results {
		processed++

		if err != nil {
			errors = append(errors, err.Error())
		}

		// Update progress
		currentProgress := len(files) - len(filesNeedingHash) + processed
		progress.UpdateProgress(currentProgress, fmt.Sprintf("í•´ì‹œ ê³„ì‚° ì¤‘... (%d/%d)", currentProgress, len(files)))
		uc.progressService.UpdateOperation(ctx, progress.ID, currentProgress, progress.CurrentStep)

		// Call progress callback
		if callback != nil {
			callback(progress)
		}

		// Log progress
		if processed%100 == 0 || processed == len(filesNeedingHash) {
			log.Printf("ğŸ“ˆ í•´ì‹œ ê³„ì‚° ì§„í–‰: %d/%d", processed, len(filesNeedingHash))
		}
	}

	if len(errors) > 0 {
		log.Printf("âš ï¸ %dê°œ íŒŒì¼ì˜ í•´ì‹œ ê³„ì‚° ì‹¤íŒ¨", len(errors))
		return fmt.Errorf("%dê°œ íŒŒì¼ì˜ í•´ì‹œ ê³„ì‚° ì‹¤íŒ¨", len(errors))
	}

	log.Printf("âœ… í•´ì‹œ ê³„ì‚° ì™„ë£Œ: %dê°œ íŒŒì¼", len(filesNeedingHash))
	return nil
}

// calculateFileHash calculates hash for a single file
func (uc *FolderComparisonUseCase) calculateFileHash(ctx context.Context, file *entities.File) error {
	hash, err := uc.hashService.CalculateFileHash(ctx, file)
	if err != nil {
		return fmt.Errorf("íŒŒì¼ í•´ì‹œ ê³„ì‚° ì‹¤íŒ¨ [%s]: %w", file.ID, err)
	}

	// Update file with hash
	file.SetHash(hash)

	// Save to database
	return uc.fileRepo.UpdateHash(ctx, file.ID, hash)
}

// findDuplicatesBetweenFolders finds duplicate files between source and target folders
func (uc *FolderComparisonUseCase) findDuplicatesBetweenFolders(sourceFiles, targetFiles []*entities.File, deepComparison bool) []*entities.File {
	// Create hash map of source files
	sourceHashes := make(map[string]*entities.File)
	sourceNameSizes := make(map[string]*entities.File) // fallback for non-hashed files

	for _, file := range sourceFiles {
		if deepComparison && file.IsHashCalculated() {
			sourceHashes[file.Hash] = file
		} else {
			// Use name + size as key for basic comparison
			key := fmt.Sprintf("%s_%d", file.Name, file.Size)
			sourceNameSizes[key] = file
		}
	}

	// Find duplicates in target files
	var duplicates []*entities.File

	for _, targetFile := range targetFiles {
		isDuplicate := false

		if deepComparison && targetFile.IsHashCalculated() {
			// Hash-based comparison (more accurate)
			if _, exists := sourceHashes[targetFile.Hash]; exists {
				isDuplicate = true
			}
		} else {
			// Name + size comparison (fallback)
			key := fmt.Sprintf("%s_%d", targetFile.Name, targetFile.Size)
			if _, exists := sourceNameSizes[key]; exists {
				isDuplicate = true
			}
		}

		if isDuplicate {
			duplicates = append(duplicates, targetFile)
		}
	}

	return duplicates
}

// DeleteTargetFolderRequest represents the request for deleting target folder
type DeleteTargetFolderRequest struct {
	ComparisonID         int                      `json:"comparisonId"`
	TargetFolderID       string                   `json:"targetFolderId"`
	DeleteEmptyFolders   bool                     `json:"deleteEmptyFolders"`
	ProgressCallback     func(*entities.Progress) `json:"-"`
	DeletionCallback     func(string, string)     `json:"-"` // fileId, status
}

// DeleteDuplicateFilesRequest represents the request for deleting duplicate files
type DeleteDuplicateFilesRequest struct {
	ComparisonID         int                      `json:"comparisonId"`
	FileIDs              []string                 `json:"fileIds"`
	DeleteEmptyFolders   bool                     `json:"deleteEmptyFolders"`
	ProgressCallback     func(*entities.Progress) `json:"-"`
	DeletionCallback     func(string, string)     `json:"-"` // fileId, status
}

// DeleteTargetFolderResponse represents the response for deleting target folder
type DeleteTargetFolderResponse struct {
	Progress         *entities.Progress `json:"progress"`
	DeletedFiles     []string           `json:"deletedFiles"`
	DeletedFolders   []string           `json:"deletedFolders"`
	FailedFiles      []string           `json:"failedFiles"`
	TotalDeleted     int                `json:"totalDeleted"`
	Errors           []string           `json:"errors,omitempty"`
}

// DeleteDuplicateFilesResponse represents the response for deleting duplicate files
type DeleteDuplicateFilesResponse struct {
	Progress         *entities.Progress `json:"progress"`
	DeletedFiles     []string           `json:"deletedFiles"`
	DeletedFolders   []string           `json:"deletedFolders"`
	FailedFiles      []string           `json:"failedFiles"`
	TotalDeleted     int                `json:"totalDeleted"`
	Errors           []string           `json:"errors,omitempty"`
}

// DeleteTargetFolder deletes the entire target folder (when 100% duplicated)
func (uc *FolderComparisonUseCase) DeleteTargetFolder(ctx context.Context, req *DeleteTargetFolderRequest) (*DeleteTargetFolderResponse, error) {
	log.Printf("ğŸ—‘ï¸ ëŒ€ìƒ í´ë” ì „ì²´ ì‚­ì œ ì‹œì‘: í´ë” ID %s", req.TargetFolderID)

	// Get comparison result to verify 100% duplication
	comparison, err := uc.comparisonRepo.GetByID(ctx, req.ComparisonID)
	if err != nil {
		return nil, fmt.Errorf("ë¹„êµ ê²°ê³¼ ì¡°íšŒ ì‹¤íŒ¨: %w", err)
	}

	if comparison == nil {
		return nil, fmt.Errorf("ë¹„êµ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: %d", req.ComparisonID)
	}

	// Verify 100% duplication
	if comparison.DuplicationPercentage < 100.0 {
		return nil, fmt.Errorf("ëŒ€ìƒ í´ë”ê°€ 100%% ì¤‘ë³µì´ ì•„ë‹™ë‹ˆë‹¤ (%.1f%% ì¤‘ë³µ)", comparison.DuplicationPercentage)
	}

	// Create progress tracker
	progress, err := uc.progressService.StartOperation(ctx, entities.OperationFileCleanup, 0)
	if err != nil {
		return nil, fmt.Errorf("ì§„í–‰ ìƒí™© ìƒì„± ì‹¤íŒ¨: %w", err)
	}

	// Initialize response
	response := &DeleteTargetFolderResponse{
		Progress:       progress,
		DeletedFiles:   make([]string, 0),
		DeletedFolders: make([]string, 0),
		FailedFiles:    make([]string, 0),
		Errors:         make([]string, 0),
	}

	// Start deletion in background
	go uc.performTargetFolderDeletion(context.Background(), req, progress, comparison, response)

	return response, nil
}

// DeleteDuplicateFiles deletes specific duplicate files from target folder
func (uc *FolderComparisonUseCase) DeleteDuplicateFiles(ctx context.Context, req *DeleteDuplicateFilesRequest) (*DeleteDuplicateFilesResponse, error) {
	log.Printf("ğŸ—‘ï¸ ì¤‘ë³µ íŒŒì¼ ì‚­ì œ ì‹œì‘: %dê°œ íŒŒì¼", len(req.FileIDs))

	// Get comparison result
	comparison, err := uc.comparisonRepo.GetByID(ctx, req.ComparisonID)
	if err != nil {
		return nil, fmt.Errorf("ë¹„êµ ê²°ê³¼ ì¡°íšŒ ì‹¤íŒ¨: %w", err)
	}

	if comparison == nil {
		return nil, fmt.Errorf("ë¹„êµ ê²°ê³¼ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: %d", req.ComparisonID)
	}

	// Create progress tracker
	progress, err := uc.progressService.StartOperation(ctx, entities.OperationFileCleanup, len(req.FileIDs))
	if err != nil {
		return nil, fmt.Errorf("ì§„í–‰ ìƒí™© ìƒì„± ì‹¤íŒ¨: %w", err)
	}

	// Initialize response
	response := &DeleteDuplicateFilesResponse{
		Progress:       progress,
		DeletedFiles:   make([]string, 0),
		DeletedFolders: make([]string, 0),
		FailedFiles:    make([]string, 0),
		Errors:         make([]string, 0),
	}

	// Start deletion in background
	go uc.performDuplicateFilesDeletion(context.Background(), req, progress, comparison, response)

	return response, nil
}

// performTargetFolderDeletion performs the actual target folder deletion
func (uc *FolderComparisonUseCase) performTargetFolderDeletion(ctx context.Context, req *DeleteTargetFolderRequest, progress *entities.Progress, comparison *entities.ComparisonResult, response *DeleteTargetFolderResponse) {
	defer func() {
		if r := recover(); r != nil {
			log.Printf("âŒ ëŒ€ìƒ í´ë” ì‚­ì œ ì¤‘ íŒ¨ë‹‰ ë°œìƒ: %v", r)
			uc.progressService.FailOperation(ctx, progress.ID, fmt.Sprintf("íŒ¨ë‹‰ ë°œìƒ: %v", r))
		}
	}()

	// Update progress to running
	progress.Start()
	uc.progressService.UpdateOperation(ctx, progress.ID, 0, "ëŒ€ìƒ í´ë” ì‚­ì œ ì‹œì‘...")

	// Delete the target folder directly
	err := uc.storageProvider.DeleteFolder(ctx, req.TargetFolderID)
	if err != nil {
		log.Printf("âŒ ëŒ€ìƒ í´ë” ì‚­ì œ ì‹¤íŒ¨: %v", err)
		uc.progressService.FailOperation(ctx, progress.ID, fmt.Sprintf("ëŒ€ìƒ í´ë” ì‚­ì œ ì‹¤íŒ¨: %v", err))
		response.Errors = append(response.Errors, fmt.Sprintf("ëŒ€ìƒ í´ë” ì‚­ì œ ì‹¤íŒ¨: %v", err))
		return
	}

	// Update response
	response.DeletedFolders = append(response.DeletedFolders, req.TargetFolderID)
	response.TotalDeleted = 1

	// Call deletion callback
	if req.DeletionCallback != nil {
		req.DeletionCallback(req.TargetFolderID, "deleted")
	}

	// Complete the operation
	uc.progressService.CompleteOperation(ctx, progress.ID)
	progress.Complete()

	if req.ProgressCallback != nil {
		req.ProgressCallback(progress)
	}

	log.Printf("âœ… ëŒ€ìƒ í´ë” ì‚­ì œ ì™„ë£Œ: %s", req.TargetFolderID)
}

// performDuplicateFilesDeletion performs the actual duplicate files deletion
func (uc *FolderComparisonUseCase) performDuplicateFilesDeletion(ctx context.Context, req *DeleteDuplicateFilesRequest, progress *entities.Progress, comparison *entities.ComparisonResult, response *DeleteDuplicateFilesResponse) {
	defer func() {
		if r := recover(); r != nil {
			log.Printf("âŒ ì¤‘ë³µ íŒŒì¼ ì‚­ì œ ì¤‘ íŒ¨ë‹‰ ë°œìƒ: %v", r)
			uc.progressService.FailOperation(ctx, progress.ID, fmt.Sprintf("íŒ¨ë‹‰ ë°œìƒ: %v", r))
		}
	}()

	// Update progress to running
	progress.Start()
	uc.progressService.UpdateOperation(ctx, progress.ID, 0, "ì¤‘ë³µ íŒŒì¼ ì‚­ì œ ì‹œì‘...")

	// Track folders that might become empty
	affectedFolders := make(map[string]bool)

	// Pre-collect parent folders for empty folder cleanup
	if req.DeleteEmptyFolders {
		log.Printf("ğŸ“ ë¹ˆ í´ë” ì •ë¦¬ë¥¼ ìœ„í•œ ë¶€ëª¨ í´ë” ì •ë³´ ìˆ˜ì§‘ ì¤‘...")
		uc.collectParentFoldersFromComparison(ctx, comparison, req.FileIDs, affectedFolders)
	}

	// Use batch deletion with parallel processing (configurable)
	batchSize := 10 // Default batch size
	progressUpdateInterval := 5 // Default progress update interval
	
	// Use configuration if available (you'll need to inject config into UseCase)
	// For now, use defaults but make them configurable later
	totalFiles := len(req.FileIDs)
	
	log.Printf("ğŸš€ ë³‘ë ¬ íŒŒì¼ ì‚­ì œ ì‹œì‘: %dê°œ íŒŒì¼, ë°°ì¹˜ í¬ê¸°: %d", totalFiles, batchSize)

	for i := 0; i < totalFiles; i += batchSize {
		end := i + batchSize
		if end > totalFiles {
			end = totalFiles
		}
		
		batch := req.FileIDs[i:end]
		uc.deleteBatchFiles(ctx, batch, req, response)
		
		// Update progress less frequently (per batch instead of per file)
		progress.UpdateProgress(end, fmt.Sprintf("íŒŒì¼ ì‚­ì œ ì¤‘... (%d/%d)", end, totalFiles))
		
		// Only update database every N batches or at end (configurable)
		if (i/batchSize)%progressUpdateInterval == 0 || end == totalFiles {
			uc.progressService.UpdateOperation(ctx, progress.ID, end, progress.CurrentStep)
		}

		// Call progress callback
		if req.ProgressCallback != nil {
			req.ProgressCallback(progress)
		}
	}

	// Delete empty folders if requested
	if req.DeleteEmptyFolders && len(affectedFolders) > 0 {
		uc.progressService.UpdateOperation(ctx, progress.ID, len(req.FileIDs), "ë¹ˆ í´ë” ì •ë¦¬ ì¤‘...")
		log.Printf("ğŸ§¹ ë¹ˆ í´ë” ì •ë¦¬ ì‹œì‘: %dê°œ í´ë” í™•ì¸", len(affectedFolders))

		deletedFolders := uc.cleanupEmptyFolders(ctx, affectedFolders)
		response.DeletedFolders = append(response.DeletedFolders, deletedFolders...)
		
		if len(deletedFolders) > 0 {
			log.Printf("âœ… %dê°œ ë¹ˆ í´ë” ì •ë¦¬ ì™„ë£Œ", len(deletedFolders))
		}
	}

	// Complete the operation
	uc.progressService.CompleteOperation(ctx, progress.ID)
	progress.Complete()

	if req.ProgressCallback != nil {
		req.ProgressCallback(progress)
	}

	log.Printf("âœ… ì¤‘ë³µ íŒŒì¼ ì‚­ì œ ì™„ë£Œ: %dê°œ ì„±ê³µ, %dê°œ ì‹¤íŒ¨", len(response.DeletedFiles), len(response.FailedFiles))
}

// cleanupEmptyFolders removes empty folders
func (uc *FolderComparisonUseCase) cleanupEmptyFolders(ctx context.Context, folderIDs map[string]bool) []string {
	var deletedFolders []string

	for folderID := range folderIDs {
		// Check if folder is empty
		files, err := uc.storageProvider.ListFiles(ctx, folderID)
		if err != nil {
			log.Printf("âš ï¸ í´ë” ë‚´ìš© í™•ì¸ ì‹¤íŒ¨ [%s]: %v", folderID, err)
			continue
		}

		if len(files) == 0 {
			// Folder is empty, delete it
			log.Printf("ğŸ—‘ï¸ ë¹ˆ í´ë” ì‚­ì œ: %s", folderID)
			err := uc.storageProvider.DeleteFolder(ctx, folderID)
			if err != nil {
				log.Printf("âŒ ë¹ˆ í´ë” ì‚­ì œ ì‹¤íŒ¨ [%s]: %v", folderID, err)
			} else {
				log.Printf("âœ… ë¹ˆ í´ë” ì‚­ì œ ì™„ë£Œ: %s", folderID)
				deletedFolders = append(deletedFolders, folderID)
			}
		} else {
			log.Printf("ğŸ“ í´ë”ê°€ ë¹„ì–´ìˆì§€ ì•ŠìŒ [%s]: %dê°œ íŒŒì¼", folderID, len(files))
		}
	}

	return deletedFolders
}

// collectParentFoldersFromComparison collects parent folder information from comparison result
func (uc *FolderComparisonUseCase) collectParentFoldersFromComparison(ctx context.Context, comparison *entities.ComparisonResult, fileIDs []string, affectedFolders map[string]bool) {
	// Create a map of file IDs to delete for quick lookup
	deleteFileMap := make(map[string]bool)
	for _, fileID := range fileIDs {
		deleteFileMap[fileID] = true
	}
	
	// Extract parent folder information from duplicate files in comparison result
	for _, file := range comparison.DuplicateFiles {
		if deleteFileMap[file.ID] && len(file.Parents) > 0 {
			affectedFolders[file.Parents[0]] = true
		}
	}
	
	log.Printf("ğŸ“ ìˆ˜ì§‘ëœ ë¶€ëª¨ í´ë” %dê°œ (íŒŒì¼ ë©”íƒ€ë°ì´í„°ì—ì„œ)", len(affectedFolders))
}

// deleteBatchFiles deletes a batch of files concurrently
func (uc *FolderComparisonUseCase) deleteBatchFiles(ctx context.Context, fileIDs []string, req *DeleteDuplicateFilesRequest, response *DeleteDuplicateFilesResponse) {
	// Use goroutines for concurrent deletion
	jobs := make(chan string, len(fileIDs))
	results := make(chan deleteResult, len(fileIDs))
	
	// Worker pool for parallel deletion
	const numWorkers = 5 // Limit concurrent deletions to avoid rate limits
	for w := 0; w < numWorkers; w++ {
		go func() {
			for fileID := range jobs {
				result := uc.deleteFileWithCallback(ctx, fileID, req.DeletionCallback)
				results <- result
			}
		}()
	}
	
	// Send jobs
	for _, fileID := range fileIDs {
		jobs <- fileID
	}
	close(jobs)
	
	// Collect results
	for range fileIDs {
		result := <-results
		
		if result.err != nil {
			log.Printf("âŒ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨ [%s]: %v", result.fileID, result.err)
			response.FailedFiles = append(response.FailedFiles, result.fileID)
			response.Errors = append(response.Errors, fmt.Sprintf("íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨ [%s]: %v", result.fileID, result.err))
		} else {
			log.Printf("âœ… íŒŒì¼ ì‚­ì œ ì™„ë£Œ: %s", result.fileID)
			response.DeletedFiles = append(response.DeletedFiles, result.fileID)
			response.TotalDeleted++
		}
	}
}

// deleteResult represents the result of a single file deletion
type deleteResult struct {
	fileID string
	err    error
}

// deleteFileWithCallback deletes a single file with callback notifications
func (uc *FolderComparisonUseCase) deleteFileWithCallback(ctx context.Context, fileID string, callback func(string, string)) deleteResult {
	// Call deletion callback - mark as deleting
	if callback != nil {
		callback(fileID, "deleting")
	}
	
	// Delete file
	err := uc.storageProvider.DeleteFile(ctx, fileID)
	
	// Call deletion callback with result
	if callback != nil {
		if err != nil {
			callback(fileID, "failed")
		} else {
			callback(fileID, "deleted")
		}
	}
	
	return deleteResult{
		fileID: fileID,
		err:    err,
	}
}

// saveFilesToDatabase saves file metadata to database to satisfy foreign key constraints
func (uc *FolderComparisonUseCase) saveFilesToDatabase(ctx context.Context, files []*entities.File) error {
	if len(files) == 0 {
		return nil
	}

	log.Printf("ğŸ’¾ ë°ì´í„°ë² ì´ìŠ¤ì— %dê°œ íŒŒì¼ ë©”íƒ€ë°ì´í„° ì €ì¥", len(files))
	
	// Save files in batches to avoid overwhelming the database
	const batchSize = 100
	for i := 0; i < len(files); i += batchSize {
		end := i + batchSize
		if end > len(files) {
			end = len(files)
		}
		
		batch := files[i:end]
		for _, file := range batch {
			// Use upsert to handle duplicates gracefully
			err := uc.fileRepo.Save(ctx, file)
			if err != nil {
				// Log error but continue with other files
				log.Printf("âš ï¸ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨ [%s]: %v", file.ID, err)
				continue
			}
		}
		
		log.Printf("ğŸ“ ë°°ì¹˜ ì €ì¥ ì™„ë£Œ: %d/%d", end, len(files))
	}
	
	log.Printf("âœ… ëª¨ë“  íŒŒì¼ ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ")
	return nil
}

// ExtractFolderIdFromUrl extracts Google Drive folder ID from URL
func (uc *FolderComparisonUseCase) ExtractFolderIdFromUrl(url string) (string, error) {
	// Google Drive folder URL patterns:
	// https://drive.google.com/drive/folders/FOLDER_ID
	// https://drive.google.com/drive/u/0/folders/FOLDER_ID
	// https://drive.google.com/open?id=FOLDER_ID
	
	log.Printf("ğŸ” Extracting folder ID from URL: %s", url)
	
	// Try different patterns - Google Drive IDs can contain letters, numbers, underscores, hyphens
	patterns := []string{
		`/folders/([a-zA-Z0-9_-]+)(?:[/?#]|$)`,  // More precise pattern with end boundary
		`[?&]id=([a-zA-Z0-9_-]+)(?:[&]|$)`,      // More precise pattern for query parameter
	}

	for _, pattern := range patterns {
		if matches := regexp.MustCompile(pattern).FindStringSubmatch(url); len(matches) > 1 {
			folderID := matches[1]
			log.Printf("âœ… Extracted folder ID: %s", folderID)
			return folderID, nil
		}
	}

	// If it's already just an ID, return as is (Google Drive IDs are typically 28-44 characters)
	if regexp.MustCompile(`^[a-zA-Z0-9_-]{10,}$`).MatchString(url) {
		log.Printf("âœ… Input is already a folder ID: %s", url)
		return url, nil
	}

	log.Printf("âŒ Failed to extract folder ID from URL: %s", url)
	return "", fmt.Errorf("Google Drive í´ë” URLì—ì„œ IDë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: %s", url)
}

// FindDuplicatesInSingleFolderRequest represents the request for finding duplicates in a single folder
type FindDuplicatesInSingleFolderRequest struct {
	FolderID          string `json:"folderId"`
	IncludeSubfolders bool   `json:"includeSubfolders"`
	MinFileSize       int64  `json:"minFileSize"`
	ForceNewScan      bool   `json:"forceNewScan"`
}

// FindDuplicatesInSingleFolderResponse represents the response for single folder duplicate finding
type FindDuplicatesInSingleFolderResponse struct {
	Progress        *entities.Progress         `json:"progress"`
	DuplicateGroups []*entities.DuplicateGroup `json:"duplicateGroups,omitempty"`
	TotalFiles      int                        `json:"totalFiles"`
	DuplicateFiles  int                        `json:"duplicateFiles"`
	WastedSpace     int64                      `json:"wastedSpace"`
	Errors          []string                   `json:"errors,omitempty"`
}

// FindDuplicatesInSingleFolder finds duplicate files within a single folder
func (uc *FolderComparisonUseCase) FindDuplicatesInSingleFolder(ctx context.Context, req *FindDuplicatesInSingleFolderRequest) (*FindDuplicatesInSingleFolderResponse, error) {
	log.Printf("ğŸ“ ë‹¨ì¼ í´ë” ë‚´ ì¤‘ë³µ íŒŒì¼ ê²€ìƒ‰ ì‹œì‘: %s", req.FolderID)

	// Create progress tracker
	progress, err := uc.progressService.StartOperation(ctx, "single_folder_duplicates", 0)
	if err != nil {
		return nil, fmt.Errorf("ì§„í–‰ ìƒí™© ìƒì„± ì‹¤íŒ¨: %w", err)
	}

	// Set metadata for checkpoint
	progress.SetMetadata("folderId", req.FolderID)
	progress.SetMetadata("includeSubfolders", req.IncludeSubfolders)
	progress.SetMetadata("minFileSize", req.MinFileSize)
	progress.SetMetadata("currentPhase", "initialized")

	// Initialize response
	response := &FindDuplicatesInSingleFolderResponse{
		Progress: progress,
		Errors:   make([]string, 0),
	}

	// Start scanning in background
	go uc.performSingleFolderDuplicateScan(context.Background(), req, progress, response)

	return response, nil
}

// performSingleFolderDuplicateScan performs the actual duplicate scanning in background
func (uc *FolderComparisonUseCase) performSingleFolderDuplicateScan(ctx context.Context, req *FindDuplicatesInSingleFolderRequest, progress *entities.Progress, response *FindDuplicatesInSingleFolderResponse) {
	defer func() {
		log.Printf("ğŸ”š ë°±ê·¸ë¼ìš´ë“œ ë‹¨ì¼ í´ë” ì¤‘ë³µ ê²€ìƒ‰ ì‘ì—… ì¢…ë£Œ - Progress ID: %d", progress.ID)
	}()

	// Phase 1: Scan folder for files
	progress.SetMetadata("currentPhase", "scanning_files")
	uc.progressService.UpdateOperation(ctx, progress.ID, 0, "í´ë” íŒŒì¼ ìŠ¤ìº” ì¤‘...")

	files, err := uc.getFilesRecursive(ctx, req.FolderID, req.IncludeSubfolders)
	if err != nil {
		log.Printf("âŒ í´ë” íŒŒì¼ ìŠ¤ìº” ì‹¤íŒ¨: %v", err)
		uc.progressService.FailOperation(ctx, progress.ID, fmt.Sprintf("í´ë” íŒŒì¼ ìŠ¤ìº” ì‹¤íŒ¨: %v", err))
		return
	}

	log.Printf("ğŸ“Š ìŠ¤ìº” ì™„ë£Œ: %dê°œ íŒŒì¼ ë°œê²¬", len(files))
	response.TotalFiles = len(files)

	// Filter files by size if specified
	if req.MinFileSize > 0 {
		filteredFiles := make([]*entities.File, 0)
		for _, file := range files {
			if file.Size >= req.MinFileSize {
				filteredFiles = append(filteredFiles, file)
			}
		}
		files = filteredFiles
		log.Printf("ğŸ“ í¬ê¸° í•„í„° ì ìš©: %dê°œ íŒŒì¼ (ìµœì†Œ %d bytes)", len(files), req.MinFileSize)
	}

	if len(files) == 0 {
		log.Printf("âš ï¸ ìŠ¤ìº”í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
		uc.progressService.CompleteOperation(ctx, progress.ID)
		return
	}

	// Phase 2: Save files to database for hash calculation
	progress.SetMetadata("currentPhase", "saving_files")
	uc.progressService.UpdateOperation(ctx, progress.ID, 0, "íŒŒì¼ ë©”íƒ€ë°ì´í„° ì €ì¥ ì¤‘...")

	err = uc.saveFilesToDatabase(ctx, files)
	if err != nil {
		log.Printf("âŒ íŒŒì¼ ë©”íƒ€ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: %v", err)
		uc.progressService.FailOperation(ctx, progress.ID, fmt.Sprintf("íŒŒì¼ ë©”íƒ€ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: %v", err))
		return
	}

	// Phase 3: Calculate hashes and find duplicates
	progress.SetMetadata("currentPhase", "calculating_hashes")
	progress.TotalItems = len(files)
	uc.progressService.UpdateOperation(ctx, progress.ID, 0, "íŒŒì¼ í•´ì‹œ ê³„ì‚° ë° ì¤‘ë³µ ê²€ìƒ‰ ì¤‘...")

	duplicateGroups, err := uc.findDuplicatesWithHashes(ctx, files, progress)
	if err != nil {
		log.Printf("âŒ ì¤‘ë³µ íŒŒì¼ ê²€ìƒ‰ ì‹¤íŒ¨: %v", err)
		uc.progressService.FailOperation(ctx, progress.ID, fmt.Sprintf("ì¤‘ë³µ íŒŒì¼ ê²€ìƒ‰ ì‹¤íŒ¨: %v", err))
		return
	}

	// Calculate statistics
	totalDuplicateFiles := 0
	wastedSpace := int64(0)
	for _, group := range duplicateGroups {
		if group.Count > 1 {
			totalDuplicateFiles += group.Count
			wastedSpace += int64(group.Count-1) * group.Files[0].Size
		}
	}

	response.DuplicateGroups = duplicateGroups
	response.DuplicateFiles = totalDuplicateFiles
	response.WastedSpace = wastedSpace

	log.Printf("âœ… ë‹¨ì¼ í´ë” ì¤‘ë³µ ê²€ìƒ‰ ì™„ë£Œ: %dê°œ ì¤‘ë³µ ê·¸ë£¹, %dê°œ ì¤‘ë³µ íŒŒì¼, %d bytes ì ˆì•½ ê°€ëŠ¥", 
		len(duplicateGroups), totalDuplicateFiles, wastedSpace)

	uc.progressService.CompleteOperation(ctx, progress.ID)
}

// findDuplicatesWithHashes finds duplicate files by calculating hashes
func (uc *FolderComparisonUseCase) findDuplicatesWithHashes(ctx context.Context, files []*entities.File, progress *entities.Progress) ([]*entities.DuplicateGroup, error) {
	hashToFiles := make(map[string][]*entities.File)
	
	for i, file := range files {
		// Calculate hash if not already calculated
		if file.Hash == "" {
			hash, err := uc.hashService.CalculateFileHash(ctx, file.ID)
			if err != nil {
				log.Printf("âš ï¸ íŒŒì¼ í•´ì‹œ ê³„ì‚° ì‹¤íŒ¨ (ê±´ë„ˆëœ€): %s - %v", file.Name, err)
				continue
			}
			file.Hash = hash

			// Update file in database
			uc.fileRepo.Update(ctx, file)
		}

		// Group files by hash
		hashToFiles[file.Hash] = append(hashToFiles[file.Hash], file)

		// Update progress
		uc.progressService.UpdateOperation(ctx, progress.ID, i+1, fmt.Sprintf("í•´ì‹œ ê³„ì‚° ì¤‘... (%d/%d)", i+1, len(files)))
	}

	// Create duplicate groups from files with same hash
	duplicateGroups := make([]*entities.DuplicateGroup, 0)
	for hash, groupFiles := range hashToFiles {
		if len(groupFiles) > 1 {
			group := entities.NewDuplicateGroup(hash)
			for _, file := range groupFiles {
				group.AddFile(file)
			}
			duplicateGroups = append(duplicateGroups, group)
		}
	}

	return duplicateGroups, nil
}

// SetConfiguration sets the use case configuration
func (uc *FolderComparisonUseCase) SetConfiguration(workerCount int, includeSubfolders, deepComparison bool, minFileSize int64) {
	if workerCount > 0 {
		uc.workerCount = workerCount
	}
	uc.includeSubfolders = includeSubfolders
	uc.deepComparison = deepComparison
	if minFileSize >= 0 {
		uc.minFileSize = minFileSize
	}
}
