package main

import (
	"context"
	"crypto/sha256"
	"database/sql"
	"encoding/json"
	"fmt"
	"io"
	"log"
	"math"
	"os"
	"regexp"
	"strings"
	"sync"
	"time"

	_ "github.com/mattn/go-sqlite3"
	"golang.org/x/oauth2"
	"golang.org/x/oauth2/google"
	"google.golang.org/api/drive/v3"
	"google.golang.org/api/googleapi"
	"google.golang.org/api/option"
)

type DriveFile struct {
	ID           string   `json:"id"`
	Name         string   `json:"name"`
	Size         int64    `json:"size"`
	WebViewLink  string   `json:"webViewLink"`
	MimeType     string   `json:"mimeType"`
	ModifiedTime string   `json:"modifiedTime"`
	Hash         string   `json:"hash,omitempty"`
	Parents      []string `json:"parents,omitempty"`
	Path         string   `json:"path,omitempty"`
}

type DriveService struct {
	service *drive.Service
	db      *sql.DB
}

type ProgressData struct {
	ProcessedFiles int                     `json:"processedFiles"`
	TotalFiles     int                     `json:"totalFiles"`
	CompletedGroups int                    `json:"completedGroups"`
	CurrentGroup   int                     `json:"currentGroup"`
	Duplicates     [][]*DriveFile          `json:"duplicates"`
	LastUpdated    time.Time               `json:"lastUpdated"`
	Status         string                  `json:"status"` // "running", "paused", "completed"
	LastPageToken  string                  `json:"lastPageToken,omitempty"`
	LastPageCount  int                     `json:"lastPageCount"`
}

const (
	dbFile = "drive_duplicates.db"
)

func initDB() (*sql.DB, error) {
	log.Println("ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì¤‘...")
	db, err := sql.Open("sqlite3", dbFile)
	if err != nil {
		return nil, fmt.Errorf("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì˜¤ë¥˜: %v", err)
	}

	// íŒŒì¼ í…Œì´ë¸” ìƒì„±
	_, err = db.Exec(`
		CREATE TABLE IF NOT EXISTS files (
			id TEXT PRIMARY KEY,
			name TEXT NOT NULL,
			size INTEGER NOT NULL,
			web_view_link TEXT NOT NULL,
			mime_type TEXT NOT NULL,
			modified_time TEXT NOT NULL,
			hash TEXT DEFAULT '',
			hash_calculated BOOLEAN DEFAULT FALSE,
			parents TEXT DEFAULT '',
			path TEXT DEFAULT '',
			last_updated DATETIME DEFAULT CURRENT_TIMESTAMP
		)
	`)
	if err != nil {
		return nil, fmt.Errorf("files í…Œì´ë¸” ìƒì„± ì˜¤ë¥˜: %v", err)
	}

	// ì§„í–‰ ìƒíƒœ í…Œì´ë¸” ìƒì„±
	_, err = db.Exec(`
		CREATE TABLE IF NOT EXISTS progress (
			id INTEGER PRIMARY KEY,
			processed_files INTEGER DEFAULT 0,
			total_files INTEGER DEFAULT 0,
			completed_groups INTEGER DEFAULT 0,
			current_group INTEGER DEFAULT 0,
			status TEXT DEFAULT 'idle',
			last_page_token TEXT DEFAULT '',
			last_page_count INTEGER DEFAULT 0,
			last_updated DATETIME DEFAULT CURRENT_TIMESTAMP
		)
	`)
	if err != nil {
		return nil, fmt.Errorf("progress í…Œì´ë¸” ìƒì„± ì˜¤ë¥˜: %v", err)
	}

	// ì„¤ì • í…Œì´ë¸” ìƒì„±
	_, err = db.Exec(`
		CREATE TABLE IF NOT EXISTS settings (
			key TEXT PRIMARY KEY,
			value TEXT NOT NULL,
			updated_at DATETIME DEFAULT CURRENT_TIMESTAMP
		)
	`)
	if err != nil {
		return nil, fmt.Errorf("settings í…Œì´ë¸” ìƒì„± ì˜¤ë¥˜: %v", err)
	}

	// ê¸°ë³¸ ë³‘ë ¬ ì‘ì—… ê°œìˆ˜ ì„¤ì •
	_, err = db.Exec(`
		INSERT OR IGNORE INTO settings (key, value) VALUES ('max_workers', '3')
	`)
	if err != nil {
		return nil, fmt.Errorf("ê¸°ë³¸ ì„¤ì • ì¶”ê°€ ì˜¤ë¥˜: %v", err)
	}

	// ì¤‘ë³µ ê·¸ë£¹ í…Œì´ë¸” ìƒì„±
	_, err = db.Exec(`
		CREATE TABLE IF NOT EXISTS duplicate_groups (
			id INTEGER PRIMARY KEY AUTOINCREMENT,
			hash TEXT NOT NULL,
			group_size INTEGER NOT NULL,
			created_at DATETIME DEFAULT CURRENT_TIMESTAMP
		)
	`)
	if err != nil {
		return nil, fmt.Errorf("duplicate_groups í…Œì´ë¸” ìƒì„± ì˜¤ë¥˜: %v", err)
	}

	// ì¤‘ë³µ íŒŒì¼ ë§¤í•‘ í…Œì´ë¸” ìƒì„±
	_, err = db.Exec(`
		CREATE TABLE IF NOT EXISTS duplicate_files (
			group_id INTEGER,
			file_id TEXT,
			FOREIGN KEY (group_id) REFERENCES duplicate_groups(id),
			FOREIGN KEY (file_id) REFERENCES files(id)
		)
	`)
	if err != nil {
		return nil, fmt.Errorf("duplicate_files í…Œì´ë¸” ìƒì„± ì˜¤ë¥˜: %v", err)
	}

	// ë°ì´í„°ë² ì´ìŠ¤ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰
	err = migrateDatabaseSchema(db)
	if err != nil {
		return nil, fmt.Errorf("ë°ì´í„°ë² ì´ìŠ¤ ë§ˆì´ê·¸ë ˆì´ì…˜ ì˜¤ë¥˜: %v", err)
	}

	log.Println("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
	return db, nil
}

func migrateDatabaseSchema(db *sql.DB) error {
	log.Println("ğŸ”„ ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ë§ˆì´ê·¸ë ˆì´ì…˜ ì¤‘...")
	
	// files í…Œì´ë¸”ì— parents, path ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸
	rows, err := db.Query("PRAGMA table_info(files)")
	if err != nil {
		return fmt.Errorf("í…Œì´ë¸” ì •ë³´ ì¡°íšŒ ì˜¤ë¥˜: %v", err)
	}
	defer rows.Close()
	
	hasParents := false
	hasPath := false
	
	for rows.Next() {
		var cid int
		var name, dataType string
		var notNull, dfltValue, pk interface{}
		
		err := rows.Scan(&cid, &name, &dataType, &notNull, &dfltValue, &pk)
		if err != nil {
			continue
		}
		
		if name == "parents" {
			hasParents = true
		}
		if name == "path" {
			hasPath = true
		}
	}
	
	// parents ì»¬ëŸ¼ ì¶”ê°€
	if !hasParents {
		log.Println("ğŸ“ parents ì»¬ëŸ¼ ì¶”ê°€ ì¤‘...")
		_, err = db.Exec("ALTER TABLE files ADD COLUMN parents TEXT DEFAULT ''")
		if err != nil {
			return fmt.Errorf("parents ì»¬ëŸ¼ ì¶”ê°€ ì˜¤ë¥˜: %v", err)
		}
		log.Println("âœ… parents ì»¬ëŸ¼ ì¶”ê°€ ì™„ë£Œ")
	}
	
	// path ì»¬ëŸ¼ ì¶”ê°€
	if !hasPath {
		log.Println("ğŸ“ path ì»¬ëŸ¼ ì¶”ê°€ ì¤‘...")
		_, err = db.Exec("ALTER TABLE files ADD COLUMN path TEXT DEFAULT ''")
		if err != nil {
			return fmt.Errorf("path ì»¬ëŸ¼ ì¶”ê°€ ì˜¤ë¥˜: %v", err)
		}
		log.Println("âœ… path ì»¬ëŸ¼ ì¶”ê°€ ì™„ë£Œ")
	}
	
	// progress í…Œì´ë¸”ì— ìƒˆ ì»¬ëŸ¼ ì¶”ê°€
	err = migrateProgressTable(db)
	if err != nil {
		return fmt.Errorf("progress í…Œì´ë¸” ë§ˆì´ê·¸ë ˆì´ì…˜ ì˜¤ë¥˜: %v", err)
	}
	
	log.Println("âœ… ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ")
	return nil
}

func migrateProgressTable(db *sql.DB) error {
	// progress í…Œì´ë¸” êµ¬ì¡° í™•ì¸
	rows, err := db.Query("PRAGMA table_info(progress)")
	if err != nil {
		return fmt.Errorf("progress í…Œì´ë¸” ì •ë³´ ì¡°íšŒ ì˜¤ë¥˜: %v", err)
	}
	defer rows.Close()
	
	hasLastPageToken := false
	hasLastPageCount := false
	
	for rows.Next() {
		var cid int
		var name, dataType string
		var notNull, dfltValue, pk interface{}
		
		err := rows.Scan(&cid, &name, &dataType, &notNull, &dfltValue, &pk)
		if err != nil {
			continue
		}
		
		if name == "last_page_token" {
			hasLastPageToken = true
		}
		if name == "last_page_count" {
			hasLastPageCount = true
		}
	}
	
	// last_page_token ì»¬ëŸ¼ ì¶”ê°€
	if !hasLastPageToken {
		log.Println("ğŸ“ progress í…Œì´ë¸”ì— last_page_token ì»¬ëŸ¼ ì¶”ê°€ ì¤‘...")
		_, err = db.Exec("ALTER TABLE progress ADD COLUMN last_page_token TEXT DEFAULT ''")
		if err != nil {
			return fmt.Errorf("last_page_token ì»¬ëŸ¼ ì¶”ê°€ ì˜¤ë¥˜: %v", err)
		}
		log.Println("âœ… last_page_token ì»¬ëŸ¼ ì¶”ê°€ ì™„ë£Œ")
	}
	
	// last_page_count ì»¬ëŸ¼ ì¶”ê°€
	if !hasLastPageCount {
		log.Println("ğŸ“ progress í…Œì´ë¸”ì— last_page_count ì»¬ëŸ¼ ì¶”ê°€ ì¤‘...")
		_, err = db.Exec("ALTER TABLE progress ADD COLUMN last_page_count INTEGER DEFAULT 0")
		if err != nil {
			return fmt.Errorf("last_page_count ì»¬ëŸ¼ ì¶”ê°€ ì˜¤ë¥˜: %v", err)
		}
		log.Println("âœ… last_page_count ì»¬ëŸ¼ ì¶”ê°€ ì™„ë£Œ")
	}
	
	return nil
}

func NewDriveService(ctx context.Context) (*DriveService, error) {
	log.Println("ğŸ”§ OAuth ì„¤ì • íŒŒì¼ ì½ëŠ” ì¤‘...")
	config, err := getOAuthConfig()
	if err != nil {
		return nil, fmt.Errorf("OAuth ì„¤ì • ì˜¤ë¥˜: %v", err)
	}
	log.Println("âœ… OAuth ì„¤ì • íŒŒì¼ ë¡œë“œ ì™„ë£Œ")

	log.Println("ğŸ« ì•¡ì„¸ìŠ¤ í† í° í™•ì¸ ì¤‘...")
	token, err := getToken(config)
	if err != nil {
		return nil, fmt.Errorf("í† í° íšë“ ì˜¤ë¥˜: %v", err)
	}
	log.Println("âœ… ì•¡ì„¸ìŠ¤ í† í° í™•ì¸ ì™„ë£Œ")

	log.Println("ğŸŒ Google Drive ì„œë¹„ìŠ¤ ì—°ê²° ì¤‘...")
	client := config.Client(ctx, token)
	
	service, err := drive.NewService(ctx, option.WithHTTPClient(client))
	if err != nil {
		return nil, fmt.Errorf("Drive ì„œë¹„ìŠ¤ ìƒì„± ì˜¤ë¥˜: %v", err)
	}
	log.Println("âœ… Google Drive ì„œë¹„ìŠ¤ ì—°ê²° ì„±ê³µ")

	// DB ì´ˆê¸°í™”
	db, err := initDB()
	if err != nil {
		return nil, err
	}

	return &DriveService{service: service, db: db}, nil
}

func getOAuthConfig() (*oauth2.Config, error) {
	credentialsFile := "credentials.json"
	
	b, err := os.ReadFile(credentialsFile)
	if err != nil {
		return nil, fmt.Errorf("credentials.json íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: %v\nêµ¬ê¸€ í´ë¼ìš°ë“œ ì½˜ì†”ì—ì„œ OAuth 2.0 í´ë¼ì´ì–¸íŠ¸ IDë¥¼ ìƒì„±í•˜ê³  credentials.jsonìœ¼ë¡œ ì €ì¥í•˜ì„¸ìš”", err)
	}

	config, err := google.ConfigFromJSON(b, drive.DriveScope)
	if err != nil {
		return nil, fmt.Errorf("OAuth ì„¤ì • íŒŒì‹± ì˜¤ë¥˜: %v", err)
	}

	// ë°ìŠ¤í¬í†± ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ ê²½ìš° redirect URL ì„¤ì •
	if config.RedirectURL == "" {
		config.RedirectURL = "urn:ietf:wg:oauth:2.0:oob"
	}

	return config, nil
}

func getToken(config *oauth2.Config) (*oauth2.Token, error) {
	tokenFile := "token.json"
	
	token, err := tokenFromFile(tokenFile)
	if err != nil {
		token = getTokenFromWeb(config)
		saveToken(tokenFile, token)
	}
	
	return token, nil
}

func getTokenFromWeb(config *oauth2.Config) *oauth2.Token {
	authURL := config.AuthCodeURL("state-token", oauth2.AccessTypeOffline)
	log.Println("ğŸ” Google ê³„ì • ì¸ì¦ì´ í•„ìš”í•©ë‹ˆë‹¤!")
	log.Println("ğŸ“‹ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ë”°ë¼ì£¼ì„¸ìš”:")
	log.Println("1. ì•„ë˜ ë§í¬ë¥¼ ë³µì‚¬í•˜ì—¬ ë¸Œë¼ìš°ì €ì—ì„œ ì—´ê¸°")
	log.Println("2. Google ê³„ì •ìœ¼ë¡œ ë¡œê·¸ì¸")
	log.Println("3. ê¶Œí•œ í—ˆìš©")
	log.Println("4. í‘œì‹œë˜ëŠ” ì¸ì¦ ì½”ë“œë¥¼ ë³µì‚¬")
	log.Println("5. ì•„ë˜ì— ì¸ì¦ ì½”ë“œ ì…ë ¥")
	log.Println("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
	fmt.Printf("%v\n", authURL)
	log.Println("â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
	fmt.Print("ì¸ì¦ ì½”ë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”: ")

	var authCode string
	if _, err := fmt.Scan(&authCode); err != nil {
		log.Fatalf("âŒ ì¸ì¦ ì½”ë“œ ì…ë ¥ ì˜¤ë¥˜: %v", err)
	}

	log.Println("ğŸ”„ ì¸ì¦ ì½”ë“œë¥¼ í† í°ìœ¼ë¡œ êµí™˜ ì¤‘...")
	token, err := config.Exchange(context.TODO(), authCode)
	if err != nil {
		log.Fatalf("âŒ í† í° êµí™˜ ì˜¤ë¥˜: %v", err)
	}
	
	log.Println("âœ… ì¸ì¦ ì„±ê³µ! í† í°ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
	return token
}

func tokenFromFile(file string) (*oauth2.Token, error) {
	f, err := os.Open(file)
	if err != nil {
		return nil, err
	}
	defer f.Close()
	
	token := &oauth2.Token{}
	err = json.NewDecoder(f).Decode(token)
	
	return token, err
}

func saveToken(path string, token *oauth2.Token) {
	f, err := os.OpenFile(path, os.O_RDWR|os.O_CREATE|os.O_TRUNC, 0600)
	if err != nil {
		log.Fatalf("í† í° íŒŒì¼ ì €ì¥ ì˜¤ë¥˜: %v", err)
	}
	defer f.Close()
	
	json.NewEncoder(f).Encode(token)
}

func (ds *DriveService) saveFilesToDB(files []*DriveFile) error {
	log.Println("ğŸ—„ï¸ íŒŒì¼ ì •ë³´ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ ì¤‘...")
	
	tx, err := ds.db.Begin()
	if err != nil {
		return fmt.Errorf("íŠ¸ëœì­ì…˜ ì‹œì‘ ì˜¤ë¥˜: %v", err)
	}
	defer tx.Rollback()

	// í…Œì´ë¸” êµ¬ì¡° í™•ì¸
	rows, err := tx.Query("PRAGMA table_info(files)")
	if err != nil {
		return fmt.Errorf("í…Œì´ë¸” ì •ë³´ ì¡°íšŒ ì˜¤ë¥˜: %v", err)
	}
	
	hasParents := false
	hasPath := false
	
	for rows.Next() {
		var cid int
		var name, dataType string
		var notNull, dfltValue, pk interface{}
		
		err := rows.Scan(&cid, &name, &dataType, &notNull, &dfltValue, &pk)
		if err != nil {
			continue
		}
		
		if name == "parents" {
			hasParents = true
		}
		if name == "path" {
			hasPath = true
		}
	}
	rows.Close()

	// ì»¬ëŸ¼ ìœ ë¬´ì— ë”°ë¼ ë‹¤ë¥¸ INSERT ë¬¸ ì‚¬ìš©
	var stmt *sql.Stmt
	if hasParents && hasPath {
		stmt, err = tx.Prepare(`
			INSERT OR REPLACE INTO files 
			(id, name, size, web_view_link, mime_type, modified_time, parents, path, last_updated)
			VALUES (?, ?, ?, ?, ?, ?, ?, ?, CURRENT_TIMESTAMP)
		`)
	} else {
		stmt, err = tx.Prepare(`
			INSERT OR REPLACE INTO files 
			(id, name, size, web_view_link, mime_type, modified_time, last_updated)
			VALUES (?, ?, ?, ?, ?, ?, CURRENT_TIMESTAMP)
		`)
	}
	
	if err != nil {
		return fmt.Errorf("ì¤€ë¹„ëœ ë¬¸ì¥ ìƒì„± ì˜¤ë¥˜: %v", err)
	}
	defer stmt.Close()

	for _, file := range files {
		if hasParents && hasPath {
			parentsJSON, _ := json.Marshal(file.Parents)
			_, err = stmt.Exec(file.ID, file.Name, file.Size, file.WebViewLink, file.MimeType, file.ModifiedTime, string(parentsJSON), file.Path)
		} else {
			_, err = stmt.Exec(file.ID, file.Name, file.Size, file.WebViewLink, file.MimeType, file.ModifiedTime)
		}
		
		if err != nil {
			return fmt.Errorf("íŒŒì¼ ì €ì¥ ì˜¤ë¥˜: %v", err)
		}
	}

	if err = tx.Commit(); err != nil {
		return fmt.Errorf("íŠ¸ëœì­ì…˜ ì»¤ë°‹ ì˜¤ë¥˜: %v", err)
	}

	log.Printf("âœ… %dê°œ íŒŒì¼ ì •ë³´ê°€ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ë¨", len(files))
	return nil
}

func (ds *DriveService) loadFilesFromDB() ([]*DriveFile, error) {
	log.Println("ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ íŒŒì¼ ëª©ë¡ ë¡œë“œ ì¤‘...")
	
	// í…Œì´ë¸” êµ¬ì¡° í™•ì¸
	infoRows, err := ds.db.Query("PRAGMA table_info(files)")
	if err != nil {
		return nil, fmt.Errorf("í…Œì´ë¸” ì •ë³´ ì¡°íšŒ ì˜¤ë¥˜: %v", err)
	}
	
	hasParents := false
	hasPath := false
	
	for infoRows.Next() {
		var cid int
		var name, dataType string
		var notNull, dfltValue, pk interface{}
		
		err := infoRows.Scan(&cid, &name, &dataType, &notNull, &dfltValue, &pk)
		if err != nil {
			continue
		}
		
		if name == "parents" {
			hasParents = true
		}
		if name == "path" {
			hasPath = true
		}
	}
	infoRows.Close()

	// ì»¬ëŸ¼ ìœ ë¬´ì— ë”°ë¼ ë‹¤ë¥¸ SELECT ë¬¸ ì‚¬ìš©
	var query string
	if hasParents && hasPath {
		query = `
			SELECT id, name, size, web_view_link, mime_type, modified_time, hash, hash_calculated, parents, path
			FROM files
			ORDER BY size DESC
		`
	} else {
		query = `
			SELECT id, name, size, web_view_link, mime_type, modified_time, hash, hash_calculated
			FROM files
			ORDER BY size DESC
		`
	}
	
	rows, err := ds.db.Query(query)
	if err != nil {
		return nil, fmt.Errorf("íŒŒì¼ ì¡°íšŒ ì˜¤ë¥˜: %v", err)
	}
	defer rows.Close()

	var files []*DriveFile
	for rows.Next() {
		file := &DriveFile{}
		var hashCalculated bool
		
		if hasParents && hasPath {
			var parentsJSON string
			err := rows.Scan(&file.ID, &file.Name, &file.Size, &file.WebViewLink, 
							&file.MimeType, &file.ModifiedTime, &file.Hash, &hashCalculated, &parentsJSON, &file.Path)
			if err != nil {
				return nil, fmt.Errorf("íŒŒì¼ ìŠ¤ìº” ì˜¤ë¥˜: %v", err)
			}
			
			// Parents JSONì„ íŒŒì‹±
			if parentsJSON != "" {
				json.Unmarshal([]byte(parentsJSON), &file.Parents)
			}
		} else {
			err := rows.Scan(&file.ID, &file.Name, &file.Size, &file.WebViewLink, 
							&file.MimeType, &file.ModifiedTime, &file.Hash, &hashCalculated)
			if err != nil {
				return nil, fmt.Errorf("íŒŒì¼ ìŠ¤ìº” ì˜¤ë¥˜: %v", err)
			}
			file.Path = ""
			file.Parents = []string{}
		}
		
		files = append(files, file)
	}

	if len(files) > 0 {
		log.Printf("âœ… ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ %dê°œ íŒŒì¼ ë¡œë“œ ì™„ë£Œ", len(files))
	}
	return files, nil
}

func (ds *DriveService) updateFileHash(fileID, hash string) error {
	_, err := ds.db.Exec(`
		UPDATE files 
		SET hash = ?, hash_calculated = TRUE, last_updated = CURRENT_TIMESTAMP
		WHERE id = ?
	`, hash, fileID)
	return err
}

func (ds *DriveService) saveProgress(progress ProgressData) error {
	// í…Œì´ë¸” êµ¬ì¡° í™•ì¸
	rows, err := ds.db.Query("PRAGMA table_info(progress)")
	if err != nil {
		return fmt.Errorf("progress í…Œì´ë¸” ì •ë³´ ì¡°íšŒ ì˜¤ë¥˜: %v", err)
	}
	
	hasLastPageToken := false
	hasLastPageCount := false
	
	for rows.Next() {
		var cid int
		var name, dataType string
		var notNull, dfltValue, pk interface{}
		
		err := rows.Scan(&cid, &name, &dataType, &notNull, &dfltValue, &pk)
		if err != nil {
			continue
		}
		
		if name == "last_page_token" {
			hasLastPageToken = true
		}
		if name == "last_page_count" {
			hasLastPageCount = true
		}
	}
	rows.Close()
	
	// ì»¬ëŸ¼ ìœ ë¬´ì— ë”°ë¼ ë‹¤ë¥¸ INSERT ë¬¸ ì‚¬ìš©
	if hasLastPageToken && hasLastPageCount {
		_, err = ds.db.Exec(`
			INSERT OR REPLACE INTO progress (id, processed_files, total_files, completed_groups, current_group, status, last_page_token, last_page_count, last_updated)
			VALUES (1, ?, ?, ?, ?, ?, ?, ?, CURRENT_TIMESTAMP)
		`, progress.ProcessedFiles, progress.TotalFiles, progress.CompletedGroups, progress.CurrentGroup, progress.Status, progress.LastPageToken, progress.LastPageCount)
	} else {
		_, err = ds.db.Exec(`
			INSERT OR REPLACE INTO progress (id, processed_files, total_files, completed_groups, current_group, status, last_updated)
			VALUES (1, ?, ?, ?, ?, ?, CURRENT_TIMESTAMP)
		`, progress.ProcessedFiles, progress.TotalFiles, progress.CompletedGroups, progress.CurrentGroup, progress.Status)
	}
	
	return err
}

func (ds *DriveService) loadProgress() (*ProgressData, error) {
	// í…Œì´ë¸” êµ¬ì¡° í™•ì¸
	rows, err := ds.db.Query("PRAGMA table_info(progress)")
	if err != nil {
		return nil, fmt.Errorf("progress í…Œì´ë¸” ì •ë³´ ì¡°íšŒ ì˜¤ë¥˜: %v", err)
	}
	
	hasLastPageToken := false
	hasLastPageCount := false
	
	for rows.Next() {
		var cid int
		var name, dataType string
		var notNull, dfltValue, pk interface{}
		
		err := rows.Scan(&cid, &name, &dataType, &notNull, &dfltValue, &pk)
		if err != nil {
			continue
		}
		
		if name == "last_page_token" {
			hasLastPageToken = true
		}
		if name == "last_page_count" {
			hasLastPageCount = true
		}
	}
	rows.Close()
	
	var progress ProgressData
	
	// ì»¬ëŸ¼ ìœ ë¬´ì— ë”°ë¼ ë‹¤ë¥¸ SELECT ë¬¸ ì‚¬ìš©
	if hasLastPageToken && hasLastPageCount {
		err = ds.db.QueryRow(`
			SELECT processed_files, total_files, completed_groups, current_group, status, last_page_token, last_page_count, last_updated
			FROM progress WHERE id = 1
		`).Scan(&progress.ProcessedFiles, &progress.TotalFiles, &progress.CompletedGroups, 
				&progress.CurrentGroup, &progress.Status, &progress.LastPageToken, &progress.LastPageCount, &progress.LastUpdated)
	} else {
		err = ds.db.QueryRow(`
			SELECT processed_files, total_files, completed_groups, current_group, status, last_updated
			FROM progress WHERE id = 1
		`).Scan(&progress.ProcessedFiles, &progress.TotalFiles, &progress.CompletedGroups, 
				&progress.CurrentGroup, &progress.Status, &progress.LastUpdated)
		
		// ê¸°ë³¸ê°’ ì„¤ì •
		progress.LastPageToken = ""
		progress.LastPageCount = 0
	}
	
	if err == sql.ErrNoRows {
		return &ProgressData{Status: "idle"}, nil
	}
	return &progress, err
}

func (ds *DriveService) saveSingleDuplicateGroup(group []*DriveFile) error {
	if len(group) < 2 {
		return nil
	}

	log.Printf("ğŸ’¾ ì¤‘ë³µ ê·¸ë£¹ ì €ì¥ ì¤‘: %dê°œ íŒŒì¼, í•´ì‹œ: %s...", len(group), group[0].Hash[:8])

	tx, err := ds.db.Begin()
	if err != nil {
		return fmt.Errorf("íŠ¸ëœì­ì…˜ ì‹œì‘ ì˜¤ë¥˜: %v", err)
	}
	defer tx.Rollback()

	// ë™ì¼í•œ í•´ì‹œì˜ ê¸°ì¡´ ê·¸ë£¹ì´ ìˆëŠ”ì§€ í™•ì¸
	var existingGroupID int64
	err = tx.QueryRow("SELECT id FROM duplicate_groups WHERE hash = ?", group[0].Hash).Scan(&existingGroupID)
	
	if err == sql.ErrNoRows {
		// ìƒˆ ê·¸ë£¹ ìƒì„±
		log.Printf("ğŸ†• ìƒˆ ì¤‘ë³µ ê·¸ë£¹ ìƒì„±: í•´ì‹œ %s...", group[0].Hash[:8])
		result, err := tx.Exec(`
			INSERT INTO duplicate_groups (hash, group_size)
			VALUES (?, ?)
		`, group[0].Hash, len(group))
		if err != nil {
			return fmt.Errorf("ê·¸ë£¹ ìƒì„± ì˜¤ë¥˜: %v", err)
		}

		existingGroupID, err = result.LastInsertId()
		if err != nil {
			return fmt.Errorf("ê·¸ë£¹ ID ì¡°íšŒ ì˜¤ë¥˜: %v", err)
		}
		log.Printf("âœ… ìƒˆ ê·¸ë£¹ ID: %d", existingGroupID)
	} else if err != nil {
		return fmt.Errorf("ê¸°ì¡´ ê·¸ë£¹ í™•ì¸ ì˜¤ë¥˜: %v", err)
	} else {
		// ê¸°ì¡´ ê·¸ë£¹ì˜ íŒŒì¼ë“¤ ì‚­ì œ í›„ ìƒˆë¡œ ì¶”ê°€
		log.Printf("ğŸ”„ ê¸°ì¡´ ê·¸ë£¹ ì—…ë°ì´íŠ¸: ID %d", existingGroupID)
		_, err = tx.Exec("DELETE FROM duplicate_files WHERE group_id = ?", existingGroupID)
		if err != nil {
			return fmt.Errorf("ê¸°ì¡´ íŒŒì¼ ì‚­ì œ ì˜¤ë¥˜: %v", err)
		}
		
		// ê·¸ë£¹ í¬ê¸° ì—…ë°ì´íŠ¸
		_, err = tx.Exec("UPDATE duplicate_groups SET group_size = ? WHERE id = ?", len(group), existingGroupID)
		if err != nil {
			return fmt.Errorf("ê·¸ë£¹ í¬ê¸° ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: %v", err)
		}
	}

	// ê·¸ë£¹ì— íŒŒì¼ë“¤ ì¶”ê°€
	for _, file := range group {
		_, err = tx.Exec(`
			INSERT INTO duplicate_files (group_id, file_id)
			VALUES (?, ?)
		`, existingGroupID, file.ID)
		if err != nil {
			return fmt.Errorf("íŒŒì¼ ì¶”ê°€ ì˜¤ë¥˜: %v", err)
		}
	}

	if err = tx.Commit(); err != nil {
		return fmt.Errorf("íŠ¸ëœì­ì…˜ ì»¤ë°‹ ì˜¤ë¥˜: %v", err)
	}

	log.Printf("âœ… ì¤‘ë³µ ê·¸ë£¹ ì €ì¥ ì™„ë£Œ: ê·¸ë£¹ ID %d, %dê°œ íŒŒì¼", existingGroupID, len(group))
	return nil
}

func (ds *DriveService) saveDuplicateGroups(duplicates [][]*DriveFile) error {
	log.Println("ğŸ—„ï¸ ì¤‘ë³µ ê·¸ë£¹ì„ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ ì¤‘...")
	
	tx, err := ds.db.Begin()
	if err != nil {
		return fmt.Errorf("íŠ¸ëœì­ì…˜ ì‹œì‘ ì˜¤ë¥˜: %v", err)
	}
	defer tx.Rollback()

	// ê¸°ì¡´ ì¤‘ë³µ ê·¸ë£¹ ì‚­ì œ
	_, err = tx.Exec("DELETE FROM duplicate_files")
	if err != nil {
		return err
	}
	_, err = tx.Exec("DELETE FROM duplicate_groups")
	if err != nil {
		return err
	}

	for _, group := range duplicates {
		if len(group) < 2 {
			continue
		}

		// ê·¸ë£¹ ìƒì„±
		result, err := tx.Exec(`
			INSERT INTO duplicate_groups (hash, group_size)
			VALUES (?, ?)
		`, group[0].Hash, len(group))
		if err != nil {
			return err
		}

		groupID, err := result.LastInsertId()
		if err != nil {
			return err
		}

		// ê·¸ë£¹ì— íŒŒì¼ë“¤ ì¶”ê°€
		for _, file := range group {
			_, err = tx.Exec(`
				INSERT INTO duplicate_files (group_id, file_id)
				VALUES (?, ?)
			`, groupID, file.ID)
			if err != nil {
				return err
			}
		}
	}

	if err = tx.Commit(); err != nil {
		return fmt.Errorf("íŠ¸ëœì­ì…˜ ì»¤ë°‹ ì˜¤ë£Œ: %v", err)
	}

	log.Printf("âœ… %dê°œ ì¤‘ë³µ ê·¸ë£¹ì´ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ë¨", len(duplicates))
	return nil
}

func (ds *DriveService) loadDuplicateGroupsPaginated(page, limit int) ([][]*DriveFile, int, error) {
	log.Printf("ğŸ—„ï¸ í˜ì´ì§€ë„¤ì´ì…˜ëœ ì¤‘ë³µ ê·¸ë£¹ ë¡œë“œ ì¤‘... (í˜ì´ì§€ %d, í•œê³„ %d)", page, limit)
	
	// ì´ ê·¸ë£¹ ê°œìˆ˜ ì¡°íšŒ
	var totalCount int
	err := ds.db.QueryRow("SELECT COUNT(*) FROM duplicate_groups").Scan(&totalCount)
	if err != nil {
		return nil, 0, fmt.Errorf("ê·¸ë£¹ ê°œìˆ˜ ì¡°íšŒ ì˜¤ë¥˜: %v", err)
	}
	
	// OFFSET ê³„ì‚°
	offset := (page - 1) * limit
	
	rows, err := ds.db.Query(`
		SELECT dg.id, dg.hash, dg.group_size,
			   f.id, f.name, f.size, f.web_view_link, f.mime_type, f.modified_time, f.hash, f.parents, f.path
		FROM duplicate_groups dg
		JOIN duplicate_files df ON dg.id = df.group_id
		JOIN files f ON df.file_id = f.id
		WHERE dg.id IN (
			SELECT id FROM duplicate_groups 
			ORDER BY id 
			LIMIT ? OFFSET ?
		)
		ORDER BY dg.id, f.name
	`, limit, offset)
	
	if err != nil {
		return nil, 0, fmt.Errorf("ì¤‘ë³µ ê·¸ë£¹ ì¡°íšŒ ì˜¤ë¥˜: %v", err)
	}
	defer rows.Close()

	groupMap := make(map[int64][]*DriveFile)
	fileCount := 0
	for rows.Next() {
		var groupID int64
		var groupHash string
		var groupSize int
		var parentsJSON string
		file := &DriveFile{}
		
		err := rows.Scan(&groupID, &groupHash, &groupSize,
						&file.ID, &file.Name, &file.Size, &file.WebViewLink,
						&file.MimeType, &file.ModifiedTime, &file.Hash, &parentsJSON, &file.Path)
		if err != nil {
			return nil, 0, fmt.Errorf("ì¤‘ë³µ ê·¸ë£¹ ìŠ¤ìº” ì˜¤ë¥˜: %v", err)
		}
		
		// Parents JSONì„ íŒŒì‹±
		if parentsJSON != "" {
			json.Unmarshal([]byte(parentsJSON), &file.Parents)
		}
		
		groupMap[groupID] = append(groupMap[groupID], file)
		fileCount++
	}

	var duplicates [][]*DriveFile
	for groupID, group := range groupMap {
		log.Printf("ğŸ“‹ ê·¸ë£¹ ID %d: %dê°œ íŒŒì¼", groupID, len(group))
		duplicates = append(duplicates, group)
	}

	log.Printf("âœ… í˜ì´ì§€ë„¤ì´ì…˜ëœ ì¤‘ë³µ ê·¸ë£¹ ë¡œë“œ ì™„ë£Œ: %dê°œ ê·¸ë£¹ (ì´ %dê°œ ì¤‘)", len(duplicates), totalCount)
	return duplicates, totalCount, nil
}

func (ds *DriveService) loadDuplicateGroups() ([][]*DriveFile, error) {
	log.Println("ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì¤‘ë³µ ê·¸ë£¹ ë¡œë“œ ì¤‘...")
	
	// ë¨¼ì € ì¤‘ë³µ ê·¸ë£¹ ê°œìˆ˜ í™•ì¸
	var groupCount int
	err := ds.db.QueryRow("SELECT COUNT(*) FROM duplicate_groups").Scan(&groupCount)
	if err != nil {
		log.Printf("âš ï¸ ì¤‘ë³µ ê·¸ë£¹ ê°œìˆ˜ ì¡°íšŒ ì‹¤íŒ¨: %v", err)
	} else {
		log.Printf("ğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ì— %dê°œ ì¤‘ë³µ ê·¸ë£¹ ì¡´ì¬", groupCount)
	}
	
	rows, err := ds.db.Query(`
		SELECT dg.id, dg.hash, dg.group_size,
			   f.id, f.name, f.size, f.web_view_link, f.mime_type, f.modified_time, f.hash, f.parents, f.path
		FROM duplicate_groups dg
		JOIN duplicate_files df ON dg.id = df.group_id
		JOIN files f ON df.file_id = f.id
		ORDER BY dg.id, f.name
	`)
	if err != nil {
		log.Printf("âŒ ì¤‘ë³µ ê·¸ë£¹ ì¡°íšŒ ì¿¼ë¦¬ ì‹¤íŒ¨: %v", err)
		return nil, fmt.Errorf("ì¤‘ë³µ ê·¸ë£¹ ì¡°íšŒ ì˜¤ë¥˜: %v", err)
	}
	defer rows.Close()

	groupMap := make(map[int64][]*DriveFile)
	fileCount := 0
	for rows.Next() {
		var groupID int64
		var groupHash string
		var groupSize int
		var parentsJSON string
		file := &DriveFile{}
		
		err := rows.Scan(&groupID, &groupHash, &groupSize,
						&file.ID, &file.Name, &file.Size, &file.WebViewLink,
						&file.MimeType, &file.ModifiedTime, &file.Hash, &parentsJSON, &file.Path)
		if err != nil {
			log.Printf("âŒ ì¤‘ë³µ ê·¸ë£¹ ìŠ¤ìº” ì˜¤ë¥˜: %v", err)
			return nil, fmt.Errorf("ì¤‘ë³µ ê·¸ë£¹ ìŠ¤ìº” ì˜¤ë¥˜: %v", err)
		}
		
		// Parents JSONì„ íŒŒì‹±
		log.Printf("ğŸ” íŒŒì¼ %sì˜ parentsJSON: '%s'", file.Name, parentsJSON)
		if parentsJSON != "" {
			err := json.Unmarshal([]byte(parentsJSON), &file.Parents)
			if err != nil {
				log.Printf("âš ï¸ Parents JSON íŒŒì‹± ì‹¤íŒ¨: %v", err)
			} else {
				log.Printf("ğŸ“‹ íŒŒì‹±ëœ Parents: %v", file.Parents)
			}
		} else {
			log.Printf("âš ï¸ íŒŒì¼ %sì˜ parents ì •ë³´ê°€ ë¹„ì–´ìˆìŒ", file.Name)
		}
		
		groupMap[groupID] = append(groupMap[groupID], file)
		fileCount++
	}

	log.Printf("ğŸ“„ ì´ %dê°œ ì¤‘ë³µ íŒŒì¼ ë¡œë“œë¨", fileCount)

	var duplicates [][]*DriveFile
	for groupID, group := range groupMap {
		log.Printf("ğŸ“‹ ê·¸ë£¹ ID %d: %dê°œ íŒŒì¼", groupID, len(group))
		duplicates = append(duplicates, group)
	}

	log.Printf("âœ… ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ %dê°œ ì¤‘ë³µ ê·¸ë£¹ ë¡œë“œ ì™„ë£Œ", len(duplicates))
	return duplicates, nil
}

func (ds *DriveService) deleteFileFromDB(fileID string) error {
	tx, err := ds.db.Begin()
	if err != nil {
		return fmt.Errorf("íŠ¸ëœì­ì…˜ ì‹œì‘ ì˜¤ë¥˜: %v", err)
	}
	defer tx.Rollback()

	// duplicate_filesì—ì„œ ì‚­ì œ
	_, err = tx.Exec("DELETE FROM duplicate_files WHERE file_id = ?", fileID)
	if err != nil {
		return fmt.Errorf("duplicate_files ì‚­ì œ ì˜¤ë¥˜: %v", err)
	}

	// filesì—ì„œ ì‚­ì œ
	_, err = tx.Exec("DELETE FROM files WHERE id = ?", fileID)
	if err != nil {
		return fmt.Errorf("files ì‚­ì œ ì˜¤ë¥˜: %v", err)
	}

	if err = tx.Commit(); err != nil {
		return fmt.Errorf("íŠ¸ëœì­ì…˜ ì»¤ë°‹ ì˜¤ë¥˜: %v", err)
	}

	return nil
}

func (ds *DriveService) cleanupEmptyGroups() error {
	// íŒŒì¼ì´ 1ê°œ ì´í•˜ì¸ ê·¸ë£¹ë“¤ì„ ì°¾ì•„ì„œ ì‚­ì œ
	rows, err := ds.db.Query(`
		SELECT dg.id, COUNT(df.file_id) as file_count
		FROM duplicate_groups dg
		LEFT JOIN duplicate_files df ON dg.id = df.group_id
		GROUP BY dg.id
		HAVING file_count <= 1
	`)
	if err != nil {
		return fmt.Errorf("ë¹ˆ ê·¸ë£¹ ì¡°íšŒ ì˜¤ë¥˜: %v", err)
	}
	defer rows.Close()

	var emptyGroupIDs []int64
	for rows.Next() {
		var groupID int64
		var fileCount int
		if err := rows.Scan(&groupID, &fileCount); err != nil {
			return fmt.Errorf("ë¹ˆ ê·¸ë£¹ ìŠ¤ìº” ì˜¤ë¥˜: %v", err)
		}
		emptyGroupIDs = append(emptyGroupIDs, groupID)
	}

	// ë¹ˆ ê·¸ë£¹ë“¤ ì‚­ì œ
	for _, groupID := range emptyGroupIDs {
		_, err = ds.db.Exec("DELETE FROM duplicate_files WHERE group_id = ?", groupID)
		if err != nil {
			log.Printf("âš ï¸ duplicate_files ì‚­ì œ ì‹¤íŒ¨ (group_id: %d): %v", groupID, err)
		}
		_, err = ds.db.Exec("DELETE FROM duplicate_groups WHERE id = ?", groupID)
		if err != nil {
			log.Printf("âš ï¸ duplicate_groups ì‚­ì œ ì‹¤íŒ¨ (id: %d): %v", groupID, err)
		}
	}

	if len(emptyGroupIDs) > 0 {
		log.Printf("ğŸ—‘ï¸ %dê°œì˜ ë¹ˆ ì¤‘ë³µ ê·¸ë£¹ ì •ë¦¬ ì™„ë£Œ", len(emptyGroupIDs))
	}

	return nil
}

func (ds *DriveService) getMaxWorkers() int {
	var value string
	err := ds.db.QueryRow("SELECT value FROM settings WHERE key = 'max_workers'").Scan(&value)
	if err != nil {
		log.Printf("âš ï¸ max_workers ì„¤ì • ì¡°íšŒ ì‹¤íŒ¨, ê¸°ë³¸ê°’ 3 ì‚¬ìš©: %v", err)
		return 3
	}
	
	maxWorkers := 3
	if _, err := fmt.Sscanf(value, "%d", &maxWorkers); err != nil {
		log.Printf("âš ï¸ max_workers íŒŒì‹± ì‹¤íŒ¨, ê¸°ë³¸ê°’ 3 ì‚¬ìš©: %v", err)
		return 3
	}
	
	// ìµœì†Œ 1ê°œ, ìµœëŒ€ 20ê°œë¡œ ì œí•œ
	if maxWorkers < 1 {
		maxWorkers = 1
	} else if maxWorkers > 20 {
		maxWorkers = 20
	}
	
	return maxWorkers
}

func (ds *DriveService) setMaxWorkers(workers int) error {
	if workers < 1 {
		workers = 1
	} else if workers > 20 {
		workers = 20
	}
	
	_, err := ds.db.Exec(`
		INSERT OR REPLACE INTO settings (key, value, updated_at) 
		VALUES ('max_workers', ?, CURRENT_TIMESTAMP)
	`, fmt.Sprintf("%d", workers))
	
	if err != nil {
		return fmt.Errorf("max_workers ì„¤ì • ì €ì¥ ì‹¤íŒ¨: %v", err)
	}
	
	log.Printf("âš™ï¸ ë³‘ë ¬ ì‘ì—… ê°œìˆ˜ê°€ %dê°œë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤", workers)
	return nil
}

func (ds *DriveService) getAllSettings() map[string]string {
	settings := make(map[string]string)
	
	rows, err := ds.db.Query("SELECT key, value FROM settings")
	if err != nil {
		log.Printf("âš ï¸ ì„¤ì • ì¡°íšŒ ì‹¤íŒ¨: %v", err)
		return settings
	}
	defer rows.Close()
	
	for rows.Next() {
		var key, value string
		if err := rows.Scan(&key, &value); err != nil {
			continue
		}
		settings[key] = value
	}
	
	return settings
}

func (ds *DriveService) clearAllData() error {
	log.Println("ğŸ—‘ï¸ ëª¨ë“  ë°ì´í„° ì‚­ì œ ì¤‘...")
	
	_, err := ds.db.Exec("DELETE FROM duplicate_files")
	if err != nil {
		return err
	}
	_, err = ds.db.Exec("DELETE FROM duplicate_groups")
	if err != nil {
		return err
	}
	_, err = ds.db.Exec("DELETE FROM files")
	if err != nil {
		return err
	}
	_, err = ds.db.Exec("DELETE FROM progress")
	if err != nil {
		return err
	}
	
	log.Println("âœ… ëª¨ë“  ë°ì´í„° ì‚­ì œ ì™„ë£Œ")
	return nil
}

func (ds *DriveService) ListAllFiles() ([]*DriveFile, error) {
	// DBì—ì„œ íŒŒì¼ ëª©ë¡ ë¡œë“œ ì‹œë„
	dbFiles, err := ds.loadFilesFromDB()
	if err == nil && len(dbFiles) > 0 {
		log.Printf("âœ… ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ %dê°œ íŒŒì¼ ë¡œë“œ ì™„ë£Œ", len(dbFiles))
		return dbFiles, nil
	}
	log.Println("â„¹ï¸ ë°ì´í„°ë² ì´ìŠ¤ê°€ ë¹„ì–´ìˆê±°ë‚˜ ì˜¤ë¥˜ ë°œìƒ. ìƒˆë¡œ ì¡°íšŒí•©ë‹ˆë‹¤.")

	// ì§„í–‰ ìƒíƒœ í™•ì¸ (ì¬ê°œ ê°€ëŠ¥í•œì§€)
	progress, err := ds.loadProgress()
	if err == nil && progress.Status == "scanning" && progress.LastPageToken != "" {
		log.Printf("ğŸ”„ ì´ì „ ìŠ¤ìº”ì„ %d í˜ì´ì§€ë¶€í„° ì¬ê°œí•©ë‹ˆë‹¤", progress.LastPageCount+1)
		return ds.resumeFileScanning(progress)
	}

	// ìƒˆë¡œìš´ ìŠ¤ìº” ì‹œì‘
	return ds.startNewFileScanning()
}

func (ds *DriveService) startNewFileScanning() ([]*DriveFile, error) {
	var allFiles []*DriveFile
	pageToken := ""
	pageCount := 0

	log.Println("ğŸ“„ Google Drive íŒŒì¼ ëª©ë¡ì„ í˜ì´ì§€ë³„ë¡œ ì¡°íšŒí•˜ê³  ìˆìŠµë‹ˆë‹¤...")

	// ì§„í–‰ ìƒíƒœ ì´ˆê¸°í™”
	progress := ProgressData{
		Status:        "scanning",
		LastPageCount: 0,
		LastPageToken: "",
	}
	ds.saveProgress(progress)

	for {
		pageCount++
		log.Printf("ğŸ“‘ í˜ì´ì§€ %d ì¡°íšŒ ì¤‘...", pageCount)
		
		query := ds.service.Files.List().
			Q("trashed=false").
			PageSize(1000).
			Fields("nextPageToken, files(id, name, size, webViewLink, mimeType, modifiedTime, parents)")

		if pageToken != "" {
			query = query.PageToken(pageToken)
		}

		var result *drive.FileList
		err := ds.retryWithBackoff(func() error {
			var err error
			result, err = query.Do()
			return err
		})
		
		if err != nil {
			log.Printf("âŒ í˜ì´ì§€ %d ì¡°íšŒ ì‹¤íŒ¨: %v", pageCount, err)
			
			// ì§„í–‰ ìƒíƒœ ì €ì¥ (ì¬ê°œ ê°€ëŠ¥í•˜ë„ë¡)
			progress.LastPageCount = pageCount - 1
			progress.LastPageToken = pageToken
			progress.Status = "interrupted"
			ds.saveProgress(progress)
			
			// 5í˜ì´ì§€ ì´ìƒ ì„±ê³µì ìœ¼ë¡œ ì¡°íšŒí–ˆë‹¤ë©´ ë¶€ë¶„ ê²°ê³¼ë¼ë„ ë°˜í™˜
			if pageCount > 5 && len(allFiles) > 0 {
				log.Printf("âš ï¸ %dê°œ í˜ì´ì§€ê¹Œì§€ ì„±ê³µí•œ ë¶€ë¶„ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤ (ì´ %dê°œ íŒŒì¼)", pageCount-1, len(allFiles))
				
				// ë¶€ë¶„ ê²°ê³¼ë¥¼ DBì— ì €ì¥
				if err := ds.saveFilesToDB(allFiles); err != nil {
					log.Printf("âš ï¸ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹¤íŒ¨: %v", err)
				}
				
				return allFiles, nil
			}
			
			return nil, fmt.Errorf("íŒŒì¼ ëª©ë¡ ì¡°íšŒ ì˜¤ë¥˜: %v", err)
		}

		validFiles := 0
		for _, file := range result.Files {
			if file.Size > 0 {
				log.Printf("ğŸ” APIì—ì„œ ë°›ì€ íŒŒì¼: %s, Parents: %v", file.Name, file.Parents)
				driveFile := &DriveFile{
					ID:           file.Id,
					Name:         file.Name,
					Size:         file.Size,
					WebViewLink:  file.WebViewLink,
					MimeType:     file.MimeType,
					ModifiedTime: file.ModifiedTime,
					Parents:      file.Parents,
					Path:         "", // ë‚˜ì¤‘ì— í•„ìš”í•  ë•Œ ê³„ì‚°
				}
				
				allFiles = append(allFiles, driveFile)
				validFiles++
			}
		}
		
		log.Printf("âœ… í˜ì´ì§€ %d: %dê°œ íŒŒì¼ ë°œê²¬ (í¬ê¸°ê°€ ìˆëŠ” íŒŒì¼ %dê°œ)", pageCount, len(result.Files), validFiles)

		// ì§„í–‰ ìƒíƒœ ì—…ë°ì´íŠ¸
		progress.LastPageCount = pageCount
		progress.LastPageToken = result.NextPageToken
		ds.saveProgress(progress)

		// 20í˜ì´ì§€ë§ˆë‹¤ ì¤‘ê°„ ì €ì¥ (20,000ê°œ íŒŒì¼ë§ˆë‹¤)
		if pageCount%20 == 0 {
			log.Printf("ğŸ”„ ì¤‘ê°„ ì €ì¥: %dê°œ í˜ì´ì§€ê¹Œì§€ %dê°œ íŒŒì¼", pageCount, len(allFiles))
			if err := ds.saveFilesToDB(allFiles); err != nil {
				log.Printf("âš ï¸ ì¤‘ê°„ ì €ì¥ ì‹¤íŒ¨: %v", err)
			}
		}

		pageToken = result.NextPageToken
		if pageToken == "" {
			break
		}
	}

	log.Printf("ğŸ“‹ ì „ì²´ ì¡°íšŒ ì™„ë£Œ: %dê°œ í˜ì´ì§€ì—ì„œ ì´ %dê°œ íŒŒì¼", pageCount, len(allFiles))
	
	// ì™„ë£Œ ìƒíƒœë¡œ ì—…ë°ì´íŠ¸
	progress.Status = "scan_completed"
	ds.saveProgress(progress)
	
	// íŒŒì¼ ëª©ë¡ì„ DBì— ì €ì¥
	if err := ds.saveFilesToDB(allFiles); err != nil {
		log.Printf("âš ï¸ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ì‹¤íŒ¨: %v", err)
	}
	
	return allFiles, nil
}

func (ds *DriveService) resumeFileScanning(progress *ProgressData) ([]*DriveFile, error) {
	// ê¸°ì¡´ íŒŒì¼ë“¤ ë¡œë“œ
	allFiles, err := ds.loadFilesFromDB()
	if err != nil {
		log.Printf("âš ï¸ ê¸°ì¡´ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨, ìƒˆë¡œ ì‹œì‘í•©ë‹ˆë‹¤: %v", err)
		return ds.startNewFileScanning()
	}

	pageToken := progress.LastPageToken
	pageCount := progress.LastPageCount

	log.Printf("ğŸ”„ í˜ì´ì§€ %dë¶€í„° íŒŒì¼ ìŠ¤ìº”ì„ ì¬ê°œí•©ë‹ˆë‹¤ (ê¸°ì¡´ %dê°œ íŒŒì¼)", pageCount+1, len(allFiles))

	for pageToken != "" {
		pageCount++
		log.Printf("ğŸ“‘ í˜ì´ì§€ %d ì¡°íšŒ ì¤‘... (ì¬ê°œ ëª¨ë“œ)", pageCount)
		
		query := ds.service.Files.List().
			Q("trashed=false").
			PageSize(1000).
			Fields("nextPageToken, files(id, name, size, webViewLink, mimeType, modifiedTime, parents)").
			PageToken(pageToken)

		var result *drive.FileList
		err := ds.retryWithBackoff(func() error {
			var err error
			result, err = query.Do()
			return err
		})
		
		if err != nil {
			log.Printf("âŒ í˜ì´ì§€ %d ì¡°íšŒ ì‹¤íŒ¨: %v", pageCount, err)
			
			// ì§„í–‰ ìƒíƒœ ì €ì¥
			progress.LastPageCount = pageCount - 1
			progress.LastPageToken = pageToken
			progress.Status = "interrupted"
			ds.saveProgress(*progress)
			
			return nil, fmt.Errorf("íŒŒì¼ ëª©ë¡ ì¡°íšŒ ì˜¤ë¥˜: %v", err)
		}

		validFiles := 0
		var newFiles []*DriveFile
		for _, file := range result.Files {
			if file.Size > 0 {
				log.Printf("ğŸ” APIì—ì„œ ë°›ì€ íŒŒì¼ (resume): %s, Parents: %v", file.Name, file.Parents)
				driveFile := &DriveFile{
					ID:           file.Id,
					Name:         file.Name,
					Size:         file.Size,
					WebViewLink:  file.WebViewLink,
					MimeType:     file.MimeType,
					ModifiedTime: file.ModifiedTime,
					Parents:      file.Parents,
					Path:         "", // ë‚˜ì¤‘ì— í•„ìš”í•  ë•Œ ê³„ì‚°
				}
				
				allFiles = append(allFiles, driveFile)
				newFiles = append(newFiles, driveFile)
				validFiles++
			}
		}
		
		log.Printf("âœ… í˜ì´ì§€ %d: %dê°œ íŒŒì¼ ë°œê²¬ (í¬ê¸°ê°€ ìˆëŠ” íŒŒì¼ %dê°œ)", pageCount, len(result.Files), validFiles)

		// ìƒˆ íŒŒì¼ë“¤ì„ DBì— ì¶”ê°€
		if len(newFiles) > 0 {
			if err := ds.saveFilesToDB(newFiles); err != nil {
				log.Printf("âš ï¸ ìƒˆ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: %v", err)
			}
		}

		// ì§„í–‰ ìƒíƒœ ì—…ë°ì´íŠ¸
		progress.LastPageCount = pageCount
		progress.LastPageToken = result.NextPageToken
		ds.saveProgress(*progress)

		pageToken = result.NextPageToken
	}

	log.Printf("ğŸ“‹ ìŠ¤ìº” ì¬ê°œ ì™„ë£Œ: %dê°œ í˜ì´ì§€ì—ì„œ ì´ %dê°œ íŒŒì¼", pageCount, len(allFiles))
	
	// ì™„ë£Œ ìƒíƒœë¡œ ì—…ë°ì´íŠ¸
	progress.Status = "scan_completed"
	ds.saveProgress(*progress)
	
	return allFiles, nil
}

func (ds *DriveService) retryWithBackoff(operation func() error) error {
	maxRetries := 5
	baseDelay := time.Second
	maxDelay := 64 * time.Second

	for attempt := 0; attempt < maxRetries; attempt++ {
		err := operation()
		if err == nil {
			return nil
		}

		// Google API ì—ëŸ¬ ì²´í¬
		if apiErr, ok := err.(*googleapi.Error); ok {
			switch apiErr.Code {
			case 500, 502, 503, 504: // ì„œë²„ ì—ëŸ¬
				if attempt == maxRetries-1 {
					return err
				}
				delay := time.Duration(math.Pow(2, float64(attempt))) * baseDelay
				if delay > maxDelay {
					delay = maxDelay
				}
				log.Printf("âš ï¸ API ì„œë²„ ì˜¤ë¥˜ (ì‹œë„ %d/%d): %v - %v í›„ ì¬ì‹œë„", attempt+1, maxRetries, apiErr.Code, delay)
				time.Sleep(delay)
				continue
			case 429: // Rate limit exceeded
				if attempt == maxRetries-1 {
					return err
				}
				delay := time.Duration(math.Pow(2, float64(attempt))) * baseDelay
				if delay > maxDelay {
					delay = maxDelay
				}
				log.Printf("âš ï¸ API ì†ë„ ì œí•œ (ì‹œë„ %d/%d): %v í›„ ì¬ì‹œë„", attempt+1, maxRetries, delay)
				time.Sleep(delay)
				continue
			default:
				return err
			}
		}

		// ê¸°íƒ€ ì¼ì‹œì  ì˜¤ë¥˜ ì²´í¬
		errStr := strings.ToLower(err.Error())
		if strings.Contains(errStr, "timeout") || 
		   strings.Contains(errStr, "connection reset") || 
		   strings.Contains(errStr, "temporary failure") {
			if attempt == maxRetries-1 {
				return err
			}
			delay := time.Duration(math.Pow(2, float64(attempt))) * baseDelay
			if delay > maxDelay {
				delay = maxDelay
			}
			log.Printf("âš ï¸ ì¼ì‹œì  ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜ (ì‹œë„ %d/%d): %v - %v í›„ ì¬ì‹œë„", attempt+1, maxRetries, err, delay)
			time.Sleep(delay)
			continue
		}

		return err
	}

	return fmt.Errorf("ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼")
}

func (ds *DriveService) DownloadFileContent(fileID string) ([]byte, error) {
	var content []byte
	err := ds.retryWithBackoff(func() error {
		resp, err := ds.service.Files.Get(fileID).Download()
		if err != nil {
			return fmt.Errorf("íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì˜¤ë¥˜: %v", err)
		}
		defer resp.Body.Close()

		content, err = io.ReadAll(resp.Body)
		if err != nil {
			return fmt.Errorf("íŒŒì¼ ë‚´ìš© ì½ê¸° ì˜¤ë¥˜: %v", err)
		}
		return nil
	})

	return content, err
}

func calculateHash(content []byte) string {
	hash := sha256.Sum256(content)
	return fmt.Sprintf("%x", hash)
}

// í´ë” ì´ë¦„ ìºì‹œ
var folderCache = make(map[string]string)
var folderCacheMutex sync.RWMutex

func (ds *DriveService) getFilePath(fileID string, parents []string) string {
	if len(parents) == 0 {
		return "/"
	}
	
	return ds.buildFullPath(parents[0])
}

// í´ë” ì •ë³´ êµ¬ì¡°ì²´ (ìºì‹œìš©)
type FolderInfo struct {
	Name     string
	Parents  []string
}

// í´ë” ìºì‹œ (ì´ë¦„ê³¼ ë¶€ëª¨ ì •ë³´ ëª¨ë‘ í¬í•¨)
var folderInfoCache = make(map[string]*FolderInfo)

func (ds *DriveService) buildFullPath(folderID string) string {
	log.Printf("ğŸ” ê²½ë¡œ êµ¬ì„± ì‹œì‘: í´ë” ID %s", folderID)
	
	if folderID == "root" || folderID == "0AEBGrCpGtL_PUk9PVA" {
		log.Printf("ğŸ“ ë£¨íŠ¸ í´ë” ê°ì§€: %s -> /", folderID)
		return "/"
	}
	
	// ìºì‹œì—ì„œ í´ë” ì •ë³´ í™•ì¸
	folderCacheMutex.RLock()
	if folderInfo, exists := folderInfoCache[folderID]; exists {
		folderCacheMutex.RUnlock()
		log.Printf("ğŸ“‹ ìºì‹œì—ì„œ í´ë” ì •ë³´ ë°œê²¬: %s (ë¶€ëª¨: %v)", folderInfo.Name, folderInfo.Parents)
		// ë¶€ëª¨ í´ë” ê²½ë¡œ ì¬ê·€ì ìœ¼ë¡œ êµ¬ì„±
		if len(folderInfo.Parents) > 0 {
			parentPath := ds.buildFullPath(folderInfo.Parents[0])
			if parentPath == "/" {
				result := "/" + folderInfo.Name
				log.Printf("ğŸ“ ê²½ë¡œ ì™„ì„± (ë£¨íŠ¸ í•˜ìœ„): %s", result)
				return result
			}
			result := parentPath + "/" + folderInfo.Name
			log.Printf("ğŸ“ ê²½ë¡œ ì™„ì„± (ì¤‘ì²©): %s", result)
			return result
		}
		result := "/" + folderInfo.Name
		log.Printf("ğŸ“ ê²½ë¡œ ì™„ì„± (ë¶€ëª¨ ì—†ìŒ): %s", result)
		return result
	}
	folderCacheMutex.RUnlock()
	
	// ìºì‹œì— ì—†ìœ¼ë©´ API í˜¸ì¶œ
	log.Printf("ğŸŒ API í˜¸ì¶œë¡œ í´ë” ì •ë³´ ì¡°íšŒ: %s", folderID)
	folder, err := ds.service.Files.Get(folderID).Fields("id, name, parents").Do()
	if err != nil {
		log.Printf("âš ï¸ í´ë” ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨ (ID: %s): %v", folderID, err)
		return "/"
	}
	
	log.Printf("ğŸ“‹ API ì‘ë‹µ: ì´ë¦„='%s', ë¶€ëª¨=%v", folder.Name, folder.Parents)
	
	// ìºì‹œì— ì €ì¥ (ì´ë¦„ê³¼ ë¶€ëª¨ ì •ë³´ ëª¨ë‘)
	folderCacheMutex.Lock()
	folderCache[folderID] = folder.Name // ê¸°ì¡´ ìºì‹œ í˜¸í™˜ì„±
	folderInfoCache[folderID] = &FolderInfo{
		Name:    folder.Name,
		Parents: folder.Parents,
	}
	folderCacheMutex.Unlock()
	
	// ë¶€ëª¨ í´ë” ê²½ë¡œ ì¬ê·€ì ìœ¼ë¡œ êµ¬ì„±
	if len(folder.Parents) > 0 {
		parentPath := ds.buildFullPath(folder.Parents[0])
		if parentPath == "/" {
			result := "/" + folder.Name
			log.Printf("ğŸ“ ê²½ë¡œ ì™„ì„± (ë£¨íŠ¸ í•˜ìœ„): %s", result)
			return result
		}
		result := parentPath + "/" + folder.Name
		log.Printf("ğŸ“ ê²½ë¡œ ì™„ì„± (ì¤‘ì²©): %s", result)
		return result
	}
	
	result := "/" + folder.Name
	log.Printf("ğŸ“ ê²½ë¡œ ì™„ì„± (ë¶€ëª¨ ì—†ìŒ): %s", result)
	return result
}

// ì¤‘ë³µ íŒŒì¼ë“¤ì˜ ê²½ë¡œë¥¼ ê°„ë‹¨íˆ ì„¤ì • (ì‹¤ì‹œê°„ í‘œì‹œë¥¼ ìœ„í•´)
func (ds *DriveService) enrichDuplicatesWithPaths(duplicates [][]*DriveFile) [][]*DriveFile {
	log.Println("ğŸ“ ì¤‘ë³µ íŒŒì¼ ê²½ë¡œ ì„¤ì • ì¤‘...")
	
	for _, group := range duplicates {
		for _, file := range group {
			// ê²½ë¡œê°€ ë¹„ì–´ìˆìœ¼ë©´ "ê²½ë¡œ ë¯¸í™•ì¸"ìœ¼ë¡œ í‘œì‹œ
			if file.Path == "" || file.Path == "/" {
				file.Path = "ê²½ë¡œ ë¯¸í™•ì¸"
			}
		}
	}
	
	log.Println("âœ… ì¤‘ë³µ íŒŒì¼ ê¸°ë³¸ ê²½ë¡œ ì„¤ì • ì™„ë£Œ")
	return duplicates
}

// ë°ì´í„°ë² ì´ìŠ¤ì˜ ëª¨ë“  íŒŒì¼ì— parents ì •ë³´ë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” í•¨ìˆ˜
func (ds *DriveService) updateAllFileParents() error {
	log.Println("ğŸ”„ ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ë“¤ì˜ parents ì •ë³´ ì—…ë°ì´íŠ¸ ì‹œì‘...")
	
	// parents ì •ë³´ê°€ ì—†ëŠ” íŒŒì¼ë“¤ ì¡°íšŒ
	rows, err := ds.db.Query("SELECT id, name FROM files WHERE parents IS NULL OR parents = ''")
	if err != nil {
		return fmt.Errorf("íŒŒì¼ ì¡°íšŒ ì˜¤ë¥˜: %v", err)
	}
	defer rows.Close()
	
	var filesToUpdate []struct {
		ID   string
		Name string
	}
	
	for rows.Next() {
		var file struct {
			ID   string
			Name string
		}
		if err := rows.Scan(&file.ID, &file.Name); err != nil {
			continue
		}
		filesToUpdate = append(filesToUpdate, file)
	}
	
	log.Printf("ğŸ“Š ì—…ë°ì´íŠ¸í•  íŒŒì¼ ìˆ˜: %dê°œ", len(filesToUpdate))
	
	// ë°°ì¹˜ í¬ê¸° ì œí•œ
	batchSize := 50
	for i := 0; i < len(filesToUpdate); i += batchSize {
		end := i + batchSize
		if end > len(filesToUpdate) {
			end = len(filesToUpdate)
		}
		
		log.Printf("ğŸ”„ ë°°ì¹˜ %d-%d ì²˜ë¦¬ ì¤‘...", i+1, end)
		
		for j := i; j < end; j++ {
			file := filesToUpdate[j]
			
			// APIì—ì„œ parents ì •ë³´ ì¡°íšŒ
			fileInfo, err := ds.service.Files.Get(file.ID).Fields("id, name, parents").Do()
			if err != nil {
				log.Printf("âš ï¸ íŒŒì¼ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: %s", file.Name)
				continue
			}
			
			// parents ì •ë³´ ì—…ë°ì´íŠ¸
			parentsJSON, _ := json.Marshal(fileInfo.Parents)
			_, err = ds.db.Exec("UPDATE files SET parents = ? WHERE id = ?", string(parentsJSON), file.ID)
			if err != nil {
				log.Printf("âš ï¸ íŒŒì¼ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: %s", file.Name)
			}
		}
		
		// API ì œí•œì„ ìœ„í•œ ëŒ€ê¸°
		time.Sleep(time.Second)
	}
	
	log.Println("âœ… parents ì •ë³´ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
	return nil
}

// íŠ¹ì • í´ë”ì˜ íŒŒì¼ë“¤ì„ ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ í•„í„°ë§í•˜ì—¬ ê²€ìƒ‰
func (ds *DriveService) searchFilesInFolder(folderID, regexPattern string) ([]*DriveFile, error) {
	log.Printf("ğŸ” í´ë” %sì—ì„œ íŒ¨í„´ '%s'ì— ë§ëŠ” íŒŒì¼ ê²€ìƒ‰ ì¤‘...", folderID, regexPattern)
	
	// ì •ê·œí‘œí˜„ì‹ ì»´íŒŒì¼
	regex, err := regexp.Compile(regexPattern)
	if err != nil {
		return nil, fmt.Errorf("ì˜ëª»ëœ ì •ê·œí‘œí˜„ì‹: %v", err)
	}
	
	var allFiles []*DriveFile
	pageToken := ""
	
	for {
		query := fmt.Sprintf("'%s' in parents and trashed=false", folderID)
		
		result, err := ds.service.Files.List().
			Q(query).
			PageSize(1000).
			PageToken(pageToken).
			Fields("nextPageToken, files(id, name, size, webViewLink, mimeType, modifiedTime, parents)").
			Do()
			
		if err != nil {
			return nil, fmt.Errorf("í´ë” íŒŒì¼ ì¡°íšŒ ì˜¤ë¥˜: %v", err)
		}
		
		for _, file := range result.Files {
			// ì •ê·œí‘œí˜„ì‹ê³¼ ë§¤ì¹˜ë˜ëŠ” íŒŒì¼ë§Œ ì¶”ê°€
			if regex.MatchString(file.Name) {
				driveFile := &DriveFile{
					ID:           file.Id,
					Name:         file.Name,
					Size:         file.Size,
					WebViewLink:  file.WebViewLink,
					MimeType:     file.MimeType,
					ModifiedTime: file.ModifiedTime,
					Parents:      file.Parents,
				}
				allFiles = append(allFiles, driveFile)
			}
		}
		
		pageToken = result.NextPageToken
		if pageToken == "" {
			break
		}
	}
	
	log.Printf("âœ… ì´ %dê°œ íŒŒì¼ì´ íŒ¨í„´ê³¼ ì¼ì¹˜í•¨", len(allFiles))
	return allFiles, nil
}

// ì—¬ëŸ¬ íŒŒì¼ì„ ì¼ê´„ ì‚­ì œ
func (ds *DriveService) bulkDeleteFiles(fileIDs []string) (int, error) {
	log.Printf("ğŸ—‘ï¸ %dê°œ íŒŒì¼ ì¼ê´„ ì‚­ì œ ì‹œì‘...", len(fileIDs))
	
	deletedCount := 0
	
	for i, fileID := range fileIDs {
		log.Printf("ì‚­ì œ ì¤‘ (%d/%d): %s", i+1, len(fileIDs), fileID)
		
		err := ds.service.Files.Delete(fileID).Do()
		if err != nil {
			log.Printf("âš ï¸ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: %s - %v", fileID, err)
			continue
		}
		
		// ë°ì´í„°ë² ì´ìŠ¤ì—ì„œë„ ì‚­ì œ
		ds.deleteFileFromDB(fileID)
		
		deletedCount++
		
		// API ì œí•œì„ ìœ„í•œ ì§§ì€ ëŒ€ê¸°
		time.Sleep(100 * time.Millisecond)
	}
	
	log.Printf("âœ… ì¼ê´„ ì‚­ì œ ì™„ë£Œ: %dê°œ ì„±ê³µ, %dê°œ ì‹¤íŒ¨", deletedCount, len(fileIDs)-deletedCount)
	return deletedCount, nil
}

func FindDuplicates(files []*DriveFile, ds *DriveService) ([][]*DriveFile, error) {
	// ê¸°ì¡´ ì§„í–‰ ìƒíƒœ í™•ì¸
	progress, err := ds.loadProgress()
	if err != nil {
		log.Printf("âš ï¸ ì§„í–‰ ìƒíƒœ ë¡œë“œ ì‹¤íŒ¨: %v", err)
		progress = &ProgressData{Status: "idle"}
	}

	// ì´ë¯¸ ì™„ë£Œëœ ì¤‘ë³µ ê·¸ë£¹ì´ ìˆìœ¼ë©´ ë°˜í™˜
	if progress.Status == "completed" {
		log.Println("âœ… ì´ì „ì— ì™„ë£Œëœ ì¤‘ë³µ ê²€ì‚¬ ê²°ê³¼ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.")
		return ds.loadDuplicateGroups()
	}

	log.Println("ğŸ”¢ íŒŒì¼ì„ í¬ê¸°ë³„ë¡œ ê·¸ë£¹í™”í•˜ëŠ” ì¤‘...")
	sizeGroups := make(map[int64][]*DriveFile)
	
	for _, file := range files {
		sizeGroups[file.Size] = append(sizeGroups[file.Size], file)
	}

	// í¬ê¸°ê°€ ê°™ì€ íŒŒì¼ë“¤ë§Œ í•„í„°ë§
	var sizeGroupsSlice [][]*DriveFile
	for _, group := range sizeGroups {
		if len(group) >= 2 {
			sizeGroupsSlice = append(sizeGroupsSlice, group)
		}
	}

	potentialDuplicates := 0
	for _, group := range sizeGroupsSlice {
		potentialDuplicates += len(group)
	}
	log.Printf("ğŸ“Š ì ì¬ì  ì¤‘ë³µ í›„ë³´: %dê°œ íŒŒì¼ (í¬ê¸°ê°€ ê°™ì€ íŒŒì¼ë“¤)", potentialDuplicates)

	// ì§„í–‰ ìƒíƒœ ì´ˆê¸°í™”
	progress.TotalFiles = potentialDuplicates
	progress.Status = "running"
	progress.ProcessedFiles = 0
	progress.CompletedGroups = 0
	progress.CurrentGroup = 0
	ds.saveProgress(*progress)

	var duplicateGroups [][]*DriveFile
	processedFiles := 0
	
	for groupIndex, sameFiles := range sizeGroupsSlice {
		progress.CurrentGroup = groupIndex
		ds.saveProgress(*progress)

		log.Printf("ğŸ” ê·¸ë£¹ %d/%d: í¬ê¸° %d bytesì¸ íŒŒì¼ %dê°œì˜ í•´ì‹œ ê³„ì‚° ì¤‘...", 
			groupIndex+1, len(sizeGroupsSlice), sameFiles[0].Size, len(sameFiles))
		
		hashGroups := make(map[string][]*DriveFile)
		
		// ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì›Œì»¤ í’€ (ë™ì  ì„¤ì •)
		maxWorkers := ds.getMaxWorkers()
		log.Printf("âš™ï¸ ë³‘ë ¬ ì‘ì—… ê°œìˆ˜: %dê°œ", maxWorkers)
		semaphore := make(chan struct{}, maxWorkers)
		var wg sync.WaitGroup
		var mu sync.Mutex
		
		for i, file := range sameFiles {
			// ì´ë¯¸ í•´ì‹œê°€ ê³„ì‚°ëœ íŒŒì¼ì€ ê±´ë„ˆë›°ê¸°
			if file.Hash != "" {
				mu.Lock()
				hashGroups[file.Hash] = append(hashGroups[file.Hash], file)
				mu.Unlock()
				processedFiles++
				continue
			}

			wg.Add(1)
			go func(file *DriveFile, index int) {
				defer wg.Done()
				
				// ì„¸ë§ˆí¬ì–´ë¡œ ë™ì‹œ ë‹¤ìš´ë¡œë“œ ì œí•œ
				semaphore <- struct{}{}
				defer func() { <-semaphore }()
				
				log.Printf("â¬‡ï¸  íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì¤‘ (%d/%d): %s", index+1, len(sameFiles), file.Name)
				
				content, err := ds.DownloadFileContent(file.ID)
				if err != nil {
					log.Printf("âŒ íŒŒì¼ %s ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: %v", file.Name, err)
					return
				}
				
				file.Hash = calculateHash(content)
				
				// DBì— í•´ì‹œ ì €ì¥
				if err := ds.updateFileHash(file.ID, file.Hash); err != nil {
					log.Printf("âš ï¸ í•´ì‹œ ì €ì¥ ì‹¤íŒ¨: %v", err)
				}
				
				mu.Lock()
				hashGroups[file.Hash] = append(hashGroups[file.Hash], file)
				processedFiles++
				progress.ProcessedFiles = processedFiles
				mu.Unlock()
				
				log.Printf("âœ… í•´ì‹œ ê³„ì‚° ì™„ë£Œ (%d/%d): %s", processedFiles, potentialDuplicates, file.Name)
				
				// 10ê°œë§ˆë‹¤ ì§„í–‰ ìƒíƒœ ì €ì¥
				if processedFiles%10 == 0 {
					ds.saveProgress(*progress)
				}
			}(file, i)
		}
		
		wg.Wait()
		
		// í•´ì‹œë³„ ê·¸ë£¹ì—ì„œ ì¤‘ë³µ ì°¾ê¸° ë° ì¦‰ì‹œ ì €ì¥
		for hash, hashFiles := range hashGroups {
			if len(hashFiles) >= 2 {
				log.Printf("ğŸ¯ ì¤‘ë³µ ë°œê²¬! í•´ì‹œ %s... : %dê°œ íŒŒì¼", hash[:8], len(hashFiles))
				duplicateGroups = append(duplicateGroups, hashFiles)
				
				// ì¦‰ì‹œ ì¤‘ë³µ ê·¸ë£¹ì„ DBì— ì €ì¥
				if err := ds.saveSingleDuplicateGroup(hashFiles); err != nil {
					log.Printf("âš ï¸ ì¤‘ë³µ ê·¸ë£¹ ì €ì¥ ì‹¤íŒ¨: %v", err)
				}
			}
		}
		
		progress.CompletedGroups = groupIndex + 1
		ds.saveProgress(*progress)
	}

	// ì™„ë£Œ ìƒíƒœë¡œ ì—…ë°ì´íŠ¸
	progress.Status = "completed"
	progress.ProcessedFiles = processedFiles
	ds.saveProgress(*progress)

	log.Printf("ğŸ ì¤‘ë³µ ê²€ì‚¬ ì™„ë£Œ: %dê°œ íŒŒì¼ ì²˜ë¦¬ë¨", processedFiles)
	log.Printf("ğŸ“Š ì´ %dê°œ ì¤‘ë³µ ê·¸ë£¹ ë°œê²¬", len(duplicateGroups))
	return duplicateGroups, nil
}